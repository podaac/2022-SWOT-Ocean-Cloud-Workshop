[
  {
    "objectID": "Direct_Access_netCDF_simple.html",
    "href": "Direct_Access_netCDF_simple.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Welcome",
    "text": "Welcome\n\nWelcome to the 2022 ECOSTRESS Cloud Workshop hosted by NASA’s Land Processes Distributed Activate Archive (LP DAAC) with support from NASA Openscapes.\nThe workshop will take place virtually daily on April 12 and 13, 2022 from 2pm-5:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Goal\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 (v2) data products in the cloud. Learning objectives focus on how to find and access ECOSTRESS v2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS v2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\n\n\nWorkshop Description\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 data products in the cloud. Learning objectives focus on how to finda and access ECOSTRESS version 2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS version 2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\nThe workshop will demonstrate how to find, access, and download ECOSTRESS v2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA’s Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will also learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud.\n\n\nWorkshop Outcomes\nAt the end of the two days, participants should be able to find and access ECOSTRESS v2 data in the NASA Earthdata Cloud (hosted in AWS). Workshop materials will be available for future reference following the completion of the workshop/ECOSTRESS Science Team meeting\n\nNOTE: ECOSTRESS v2 data will only be available to approved individuals. Please work with Christine Lee (christine.m.lee@jpl.nasa.gov) to have your name added to the allowlist."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2022 ECOSTRESS Cloud Workshop is hosted by NASA’s LP DAAC with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "href": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud",
    "text": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#summary",
    "href": "tutorials/04_On-Prem_Cloud.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\n\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#requirements",
    "href": "tutorials/04_On-Prem_Cloud.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#learning-objectives",
    "href": "tutorials/04_On-Prem_Cloud.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "href": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project)."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Sun, 12 Dec 2021 01:52:31 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fb3944adb50>\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fb39fcd8040>"
  },
  {
    "objectID": "tutorials/Direct_Access_netCDF_simple.html",
    "href": "tutorials/Direct_Access_netCDF_simple.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "tutorials/01_Earthdata_Search.html",
    "href": "tutorials/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECOSTRESS LSTE, hosted by the LP DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left.\n\n\n\nFigure caption: Search for ECCO data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECCO monthly SSH in the search box (which will produce 39 matching collections), and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nFigure caption: Refine search, set temporal bounds, get more information\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\nthe S3 storage bucket and object prefix where this data is located\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nPro Tip: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (here again it’s the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n4.a. Entire file content\nLet’s stay we are interested in the entire file content, so we select the “Direct Download” option (as opposed to other options to subset or transform the data):\n\n\n\nFigure caption: Customize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n  \nThe Download Files tab provides the https:// links for downloading the files locally. E.g.: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud (an example will be shown in Tutorial 3). E.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\nTip: Another quicker way to find the bucket and object prefix is from the list of data files the search returns. Next to the + green button is a grey donwload symbol. Click on that to see the Download Files https:// links or on the AWS S3 Access to get the direct S3:// access links, which contain the bucket and object prefix where data is stored.\n\n\n4.b. Subset or transform before download or access\nDAAC tools and services are also being migrated or developed in the cloud, next to that data. These include the Harmony API and OPeNDAP in the cloud, as a few examples.\nWe can leverage these cloud-based services on cloud-archived data to reduce or transform the data (depending on need) before getting the access links regardless of whether we prefer to download the data and work on a local machine or whether we want to access the data in the cloud (from a cloud workspace). These can be useful data reduction services that support a faster time to science.\nHarmony\nHarmony allows you to seamlessly analyze Earth observation data from different NASA data centers. These services (API endpoints) provide data reduction (e.g. subsetting) and transfromation services (e.g. convert netCDF data to Zarr cloud optimized format).\n\n\n\nFigure caption: Leverage Harmony cloud-based data transformation services\n\n\nWhen you click the final green Download button, the links provided are to data that had been transformed based on our selections on the previous screen (here chosing to use the Harmony service to reformat the data to Zarr). These data are staged for us in an S3 bucket in AWS, and we can use the s3:// links to access those specific data. This service also provides STAC access links. This particular example is applicable if your workflow is in the AWS us-west-2 region.\n\n\n\nFigure caption: Harmony-staged data in S3\n\n\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nIn tutorial 3 Direct Data Access, we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\nTutorial 3 will pick up from here and cover these next steps in more detail."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2022-ECOSTRESS-Cloud-Workshop/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book (my teaching notes, your reference material)\nZoom Chat"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2021-Cloud-Workshop-AGU is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "further-resources.html#a-growing-list-of-resources",
    "href": "further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "A growing list of resources!",
    "text": "A growing list of resources!\n\nOne stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Open",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Open\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Authentication\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "from pystac_client import Client\nimport stackstac\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Cloud Optimized GeoTIFF (COG)",
    "text": "Cloud Optimized GeoTIFF (COG)\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\nImport Packages\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "HTTPS Data Access",
    "text": "HTTPS Data Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Outline:",
    "text": "Outline:\n\nIntroduction to NASA Earthdata’s move to the cloud\n\nBackground and motivation\nEnabling Open Science via “Analysis-in-Place”\nResources for cloud adopters: NASA Earthdata Openscapes\n\nNASA Earthdata discovery and access in the cloud\n\nPart 1: Explore Earthdata cloud data availablity\nPart 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nPart 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\n\n\n\nTutorial materials are adapted from repos on the NASA Openscapes public Github:\n\nThis notebook source code: update https://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/how-tos\nAlso available via online Quarto book: update https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "The NASA Earthdata archive continues to grow",
    "text": "The NASA Earthdata archive continues to grow\n\n\n\nEOSDIS Data Archive"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "The NASA Earthdata Cloud Evolution",
    "text": "The NASA Earthdata Cloud Evolution\n \n\nNASA Distributed Active Archive Centers (DAACs) are continuing to migrate data to the Earthdata Cloud\n\nSupporting increased data volume as new, high-resolution remote sensing missions launch in the coming years\nData hosted via Amazon Web Services, or AWS\nDAACs continuing to support tools, services, and tutorial resources for our user communities"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "NASA Earthdata Cloud as an enabler of Open Science",
    "text": "NASA Earthdata Cloud as an enabler of Open Science\n\nReducing barriers to large-scale scientific research in the era of “big data”\nIncreasing community contributions with hands-on engagement\nPromoting reproducible and shareable workflows without relying on local storage systems\n\n\n\n\nOpen Data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Data and Analysis co-located “in place”",
    "text": "Data and Analysis co-located “in place”\n\n\n\nEarthdata Cloud Paradigm"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Building NASA Earthdata Cloud Resources",
    "text": "Building NASA Earthdata Cloud Resources\nShow slide with 3 panels of user resources\nEmphasize that the following tutorials are short examples that were taken from the tutorial resources we have been building for our users"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "NASA Earthdata Cloud: Discovery and access using open source technologies",
    "text": "NASA Earthdata Cloud: Discovery and access using open source technologies\nThe following tutorial demonstrates several basic end-to-end workflows to interact with data “in-place” from the NASA Earthdata Cloud, accessing Amazon Web Services (AWS) Single Storage Solution (S3) data locations without the need to download data. While the data can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow.\nAlthough the examples we’re working with in this notebook only focuses on a small time and area for demonstration purposes, this workflow can be modified and scaled up to suit a larger time range and region of interest.\n\nDatasets of interest:\n\nHarmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002)\n\nSurface reflectance (SR) and top of atmosphere (TOA) brightness data\nGlobal observations of the land every 2–3 days at 30-meter (m)\nCloud Optimized GeoTIFF (COG) format\n\nECCO Sea Surface Height - Daily Mean 0.5 Degree (Version 4 Release 4)(10.5067/ECG5D-SSH44).\n\nDaily-averaged dynamic sea surface height\nTime series of monthly NetCDFs on a 0.5-degree latitude/longitude grid."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 1: Explore Data hosted in the Earthdata Cloud",
    "text": "Part 1: Explore Data hosted in the Earthdata Cloud\n\nEarthdata Search Demonstration\nFrom Earthdata Search https://search.earthdata.nasa.gov, use your Earthdata login credentials to log in. You can create an Earthdata Login account at https://urs.earthdata.nasa.gov.\nIn this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 39 matching collections were found with the ECCO monthly SSH search, and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box. Scroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\n\n\nView and Select Data Access Options\nClicking on the ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4) dataset, we now see a list of files (granules) that are part of the dataset (collection). We can click on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access Options",
    "text": "Access Options\nSelect the “Direct Download” option to view Access options via Direct Download and from the AWS Cloud. Additional options to customize the data are also available for this dataset.\n\n\n\nFigure caption: Customize your download or access"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Earthdata Cloud access information",
    "text": "Earthdata Cloud access information\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. The AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.\nE.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\n\nIntegrate file links into programmatic workflow, locally or in the AWS cloud.\nIn the next two examples we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\n\n\n\nFigure caption: Direct S3 access"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)",
    "text": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format archived by the Land Processes (LP) DAAC. The COGs can be used like any other GeoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this example, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Connect to the CMR-STAC API",
    "text": "Connect to the CMR-STAC API\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nprovider_cat = Client.open(STAC_URL)\n\n\nConnect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD). We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search for STAC Items",
    "text": "Search for STAC Items\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file and plot\nReading in a geojson file with geopandas and extract coodinates for our ROI.\n\nfield = geopandas.read_file('../data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]\nroi = json.loads(field.to_json())['features'][0]['geometry']\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package. STAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n\ndate_range = \"2021-05/2021-08\"\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nView STAC Items that matched our search query\n\nprint('Matching STAC Items:', search.matched())\nitem_collection = search.get_all_items()\nitem_collection[0].to_dict()\n\nMatching STAC Items: 113\n\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nBelow we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis. We will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above) and print out the first ten links, converted to s3 locations:\n\ncloudcover = 25\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n                \ns3_links = [l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://') for l in evi_band_links]\ns3_links[:10]\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access s3 storage location",
    "text": "Access s3 storage location\nAccess s3 credentials from LP.DAAC and create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ntemp_creds_req = requests.get(s3_cred_endpoint).json()\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL Configurations\nGDAL is a foundational piece of geospatial software that is leveraged by several popular open-source, and closed, geospatial software. The rasterio package is no exception. Rasterio leverages GDAL to, among other things, read and write raster data files, e.g., GeoTIFFs/Cloud Optimized GeoTIFFs. To read remote files, i.e., files/objects stored in the cloud, GDAL uses its Virtual File System API. In a perfect world, one would be able to point a Virtual File System (there are several) at a remote data asset and have the asset retrieved, but that is not always the case. GDAL has a host of configurations/environmental variables that adjust its behavior to, for example, make a request more performant or to pass AWS credentials to the distribution system. Below, we’ll identify the evironmental variables that will help us get our data from cloud\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f64510812e0>\n\n\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n# s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Read Cloud-Optimized GeoTIFF into rioxarray",
    "text": "Read Cloud-Optimized GeoTIFF into rioxarray\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot using hvplot",
    "text": "Plot using hvplot\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service",
    "text": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3). In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis example demonstrates “analysis in place” using customized ECCO Level 4 monthly sea surface height data, in this case reformatted to Zarr, from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony:\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nfrom pqdm.threads import pqdm\nimport xarray as xr"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Using Harmony-Py to customize data",
    "text": "Using Harmony-Py to customize data\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n\nharmony_client = Client()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Create Harmony Request",
    "text": "Create Harmony Request\nSpecify a temporal range over 2015, and Zarr as an output format.\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    temporal={\n        'start': dt.datetime(2015, 1, 2),\n        'stop': dt.datetime(2015, 12, 31),\n    },\n    format='application/x-zarr'\n)\n\njob_id = harmony_client.submit(request)"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Check request status and view output URLs",
    "text": "Check request status and view output URLs\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response:\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\ns3_urls = list(results)\ns3_urls\n\n [ Processing:  83% ] |##########################################         | [/]\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Open staged files with s3fs and xarray",
    "text": "Open staged files with s3fs and xarray\nAccess AWS credentials for the Harmony bucket, and use the AWS s3fs package to create a file system that can then be read by xarray. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\ncreds = harmony_client.aws_credentials()\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nOpen the Zarr stores using the s3fs package, then load them all at once into a concatenated xarray dataset:\n\nstores = [s3fs.S3Map(root=url, s3=s3_fs, check=False) for url in s3_urls]\ndef open_zarr_xarray(store):\n    return xr.open_zarr(store=store, consolidated=True)\n\ndatasets = pqdm(stores, open_zarr_xarray, n_jobs=12)\n\nds = xr.concat(datasets, 'time', coords='minimal', )\nds = xr.decode_cf(ds, mask_and_scale=True, decode_coords=True)\nds\n\n\nssh_da = ds.SSH\n\nssh_da.to_masked_array(copy=False)\n\nssh_da"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot the Sea Surface Height time series using hvplot",
    "text": "Plot the Sea Surface Height time series using hvplot\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\nssh_da = ds.SSH\n\nstdev_ssh = ssh_da.std('time')\nstdev_ssh.name = 'stdev of analysed_sst [Kelvin]'\nstdev_ssh.plot();\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Further Resources",
    "text": "Further Resources\n\nReference Hackathon/workshop tutorials that go into more detail!\nEarthdata Cloud Cookbook\nEarthdata Cloud Primer\n\nGetting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "logistics/schedule.html#workshop-schedule",
    "href": "logistics/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDay 1 - March 16, 2022\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome\nNadya/Jessica, NASA Physical Oceanography Office\n\n\n9:05 am\nTBD\nLee-Lueng Fu, SWOT Project\n\n\n9:10 am\nWorkshop Goals and Intro to the new Earthdata Cloud Paradigm\nCatalina Oaida, PO.DAAC Applied Science Systems Engineer\n\n\n9:30 am\nIntroduce the SWOT L2 simulated dataset\nJinbo Wang, PO.DAAC Project Scientist\n\n\n9:35 am\nEarthdata Authentication, Downloading (e.g. Subscriber) - Review\nSuresh Vannan, PO.DAAC Project Manager\n\n\n10:05 am\nIntro to JupyterHub in AWS (everyone logs in)\nErin Robinson, Openscapes\n\n\n10:20 am\nTutorial 1: Direct S3 Access for synthetic SWOT L2 SSH\nMike Gangl, PO.DAAC Project Systems Engineer\n\n\n10:50 am\nBreak\n\n\n\n11:00 am\nQ&A and Hack-time\nAll\n\n\n1:00 pm\nClosing for the Day\n\n\n\n\n\n\nDay 2 - March 17, 2022\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC Applied Science Systems Engineer\n\n\n9:05 am\nTutorial 2: Subsetting synthetic SWOT L2 SSH\nCelia Ou/Jack McNelis, PO.DAAC Data Publication Engineers\n\n\n9:35 am\nTutorial 3: Creating synthetic SWOT L2 time series\nTBD\n\n\n9:50 am\nQ&A / Hack-time\nAll\n\n\n11:30 am\nParticipant Reportout\nParticipants\n\n\n12:30 pm\nWhat’s Next / Save your Work\nJulie Lowndes, Openscapes\n\n\n1:00 pm\nWorkshop Concludes"
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites, Homework & Help",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nThe workshop signup form asks for your GitHub username - this allows us to enable you access to a cloud environment during the workshop.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You can follow along in Jupyter Hub on your own computer while also watching an instructor demo over WebEx (or equivalent)."
  },
  {
    "objectID": "logistics/prerequisites.html#pre-workshop-homework",
    "href": "logistics/prerequisites.html#pre-workshop-homework",
    "title": "Prerequisites, Homework & Help",
    "section": "Pre-Workshop Homework",
    "text": "Pre-Workshop Homework\nWe recommmend watching or walking through the following tutorials ahead of the workshop, to become familiar with topics that we won’t have time to take a deep dive into during the workshop, but which are important pieces in accessing (simulated) SWOT data hosted in the NASA Earthdata Cloud. You will get the chance to ask any questions regarding the material covered in these pre-workshop tutorials during the workshop.\n\nSearch for PO.DAAC data using the Earthdata Search GUI\n\nGeneric tutorial on how to search and retrieve download or access links for PO.DAAC data using the Earthdata Search GUI.\n\nNASA Earthdata Authentication\n\nTutorial for generating a netrc file, which enables programmatic (command line) NASA Earthdata Authentication. If you don’t have a NASA Earthdata user login account, you can set up a free one quickly. See the Prerequisites section above.\n\nDownload data from the NASA Earthdata Cloud using the PO.DAAC data-subscriber (12 minutes)\n\nWhile the workshop will focus on data access within the cloud, data download from the NASA Earthdata Cloud is still possbile. This tutorial will walk you through how to download simulated SWOT L2 data from the PO.DAAC archive in Earthdata Cloud (hosted in AWS), using the PO.DAAC data-subscriber tool."
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help-during-the-workshop",
    "href": "logistics/prerequisites.html#getting-help-during-the-workshop",
    "title": "Prerequisites, Homework & Help",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use WebEx Chat and Google Doc notes as our main channels for help. Please use WebEx Chat to post questions, direct helpers to any screenshots you’ve pasted or conversations in the Google Doc, and request a breakout room.\nTo create a screenshot:\n\nOn your Mac - Screenshot\nOn your PC - Snipping Tool"
  },
  {
    "objectID": "logistics/index.html#for-workshop-participants",
    "href": "logistics/index.html#for-workshop-participants",
    "title": "Logistics overview",
    "section": "For Workshop Participants",
    "text": "For Workshop Participants\nBefore the workshop, please complete the prerequisites and pre-workshop homework."
  },
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "prerequisites/01_Earthdata_Search.html",
    "href": "prerequisites/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 104 matching collections were found with the basic ECCO search.\n\n\n\nFigure caption: Search for ECCO data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECCO monthly SSH in the search box (which will produce 39 matching collections), and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nFigure caption: Refine search, set temporal bounds, get more information\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\nthe S3 storage bucket and object prefix where this data is located\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nPro Tip: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (here again it’s the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n4.a. Entire file content\nLet’s stay we are interested in the entire file content, so we select the “Direct Download” option (as opposed to other options to subset or transform the data):\n\n\n\nFigure caption: Customize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n  \nThe Download Files tab provides the https:// links for downloading the files locally. E.g.: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud (an example will be shown in Tutorial 3). E.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\nTip: Another quicker way to find the bucket and object prefix is from the list of data files the search returns. Next to the + green button is a grey donwload symbol. Click on that to see the Download Files https:// links or on the AWS S3 Access to get the direct S3:// access links, which contain the bucket and object prefix where data is stored.\n\n\n4.b. Subset or transform before download or access\nDAAC tools and services are also being migrated or developed in the cloud, next to that data. These include the Harmony API and OPeNDAP in the cloud, as a few examples.\nWe can leverage these cloud-based services on cloud-archived data to reduce or transform the data (depending on need) before getting the access links regardless of whether we prefer to download the data and work on a local machine or whether we want to access the data in the cloud (from a cloud workspace). These can be useful data reduction services that support a faster time to science.\nHarmony\nHarmony allows you to seamlessly analyze Earth observation data from different NASA data centers. These services (API endpoints) provide data reduction (e.g. subsetting) and transfromation services (e.g. convert netCDF data to Zarr cloud optimized format).\n\n\n\nFigure caption: Leverage Harmony cloud-based data transformation services\n\n\nWhen you click the final green Download button, the links provided are to data that had been transformed based on our selections on the previous screen (here chosing to use the Harmony service to reformat the data to Zarr). These data are staged for us in an S3 bucket in AWS, and we can use the s3:// links to access those specific data. This service also provides STAC access links. This particular example is applicable if your workflow is in the AWS us-west-2 region.\n\n\n\nFigure caption: Harmony-staged data in S3\n\n\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nIn tutorial 3 Direct Data Access, we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\nTutorial 3 will pick up from here and cover these next steps in more detail."
  },
  {
    "objectID": "prerequisites/index.html#prerequisites",
    "href": "prerequisites/index.html#prerequisites",
    "title": "Prerequisites",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nThe workshop signup form asks for your GitHub username - this allows us to enable you access to a cloud environment during the workshop.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\nSlack\n\nWhile we will be taking questions in person, we will also be communicating and recording questions via Slack. To take part in the conversations please ask Christine Lee or Gregory Halverson to add you to the ECOSTRESS Slack workspace."
  },
  {
    "objectID": "prerequisites/index.html#pre-workshop-homework",
    "href": "prerequisites/index.html#pre-workshop-homework",
    "title": "Prerequisites & Homework",
    "section": "Pre-Workshop Homework",
    "text": "Pre-Workshop Homework\nWe recommmend watching or walking through the following tutorials ahead of the workshop, to become familiar with topics that we won’t have time to take a deep dive into during the workshop, but which are important pieces in accessing (simulated) SWOT data hosted in the NASA Earthdata Cloud. You will get the chance to ask any questions regarding the material covered in these pre-workshop tutorials during the workshop.\n\nTutorial: Search for PO.DAAC data using the Earthdata Search GUI (15 min)\n\nGeneric tutorial on how to search and retrieve download or access links for PO.DAAC data using the Earthdata Search GUI.\n\nTutorial: NASA Earthdata Authentication (5 minutes)\n\nTutorial for generating a netrc file, which enables programmatic (command line) NASA Earthdata Authentication. If you don’t have a NASA Earthdata user login account, you can set up a free one quickly. See the Prerequisites section above.\n\nTutorial: Download data from the NASA Earthdata Cloud using the PO.DAAC data-subscriber (12 minutes)\n\nWhile the workshop will focus on data access within the cloud, data download from the NASA Earthdata Cloud is still possbile. This tutorial will walk you through how to download simulated SWOT L2 data from the PO.DAAC archive in Earthdata Cloud (hosted in AWS), using the PO.DAAC data-subscriber tool."
  },
  {
    "objectID": "prerequisites/index.html#getting-help-during-the-workshop",
    "href": "prerequisites/index.html#getting-help-during-the-workshop",
    "title": "Prerequisites, Homework & Help",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use WebEx Chat and Google Doc notes as our main channels for help. Please use WebEx Chat to post questions, direct helpers to any screenshots you’ve pasted or conversations in the Google Doc, and request a breakout room.\nTo create a screenshot:\n\nOn your Mac - Screenshot\nOn your PC - Snipping Tool"
  },
  {
    "objectID": "prerequisites/02_NASA_Earthdata_Authentication.html#summary",
    "href": "prerequisites/02_NASA_Earthdata_Authentication.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "prerequisites/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "prerequisites/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "prerequisites/download.html",
    "href": "prerequisites/download.html",
    "title": "Download with PO.DAAC data-subscriber",
    "section": "",
    "text": "TODO: youtube video link"
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDay 1 - April 12, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome / Workshop Expectations\nAaron Friesz (LP DAAC) / Christine Lee (JPL)\n\n\n2:10 pm\nECOSTRESS Build 7 collection Overview\nChristine Lee & Gregory Halverson (JPL)\n\n\n2:25 pm\nEarthdata Cloud Overview\nAaron Friesz (LP DAAC)\n\n\n2:55 pm\nBreak/QA\n\n\n\n3:00 pm\nEarthdata Search Client (GUI)\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nGetting set up / Intro to Pangeo\nAaron Friesz (LP DAAC)\n\n\n4:30 pm\nIntro to xarray & hvplot\nAaron Friesz (LP DAAC)\n\n\n5:15 pm\nBreak/QA\n\n\n\n\n\nClosing Day 1\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the workshop daily scheduled session, using JupyterHub.\n\nAgenda for tomorrow: what’s coming next.\n\n\n\n\nDay 2 - March 17, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome / Earthdata Authentication: Set up netrc file\nAaron Friesz (LP DAAC)\n\n\n2:10 pm\nEarthdata Cloud: Search and Discovery - CMR API\nAaron Friesz (LP DAAC)\n\n\n2:45 pm\nBreak/QA\n\n\n\n2:50 pm\nEarthdata Cloud: Search and Discovery - CMR STAC API\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nEarthdata Cloud: Search and Discovery - Data Access\nAaron Friesz (LP DAAC)\n\n\n4:30 pm\nE2E Workflow\nGregory Halverson (JPL)\n\n\n5:30 pm\nBreak/QA\n\n\n\n\n\nClosing Day 2\n\nThank you!\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following the ECOSTRESS Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "schedule.html#getting-help-during-the-workshop",
    "href": "schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use the ECOSTRESS Slack Workspace as our main channels for help. Please use Slack to post questions."
  },
  {
    "objectID": "prerequisites/03_download.html",
    "href": "prerequisites/03_download.html",
    "title": "03. Download with PO.DAAC data-subscriber",
    "section": "",
    "text": "Tutorial: Download data from the NASA Earthdata Cloud using the PO.DAAC data-subscriber (12 minutes)\nWhile the workshop will focus on data access within the cloud, data download from the NASA Earthdata Cloud is still possbile. This tutorial will walk you through how to download simulated SWOT L2 data from the PO.DAAC archive in Earthdata Cloud (hosted in AWS), using the PO.DAAC data-subscriber tool."
  },
  {
    "objectID": "tutorials/03_Timeseries_SWOT_sim.html",
    "href": "tutorials/03_Timeseries_SWOT_sim.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "03. Build a Time Series of SWOT Simulated Data\n\nImport libraries\nIn additional to libraries from the first tutorial, also import libraries needed to submit CMR requests.\n\nimport xarray as xr\nimport numpy as np\nfrom IPython.display import display, JSON\nfrom datetime import datetime, timedelta, time\nimport os\n\n# highlight the harmony-py library\nfrom harmony import BBox, Client, Collection, Request, Environment, LinkType \n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n%matplotlib inline\n\n# Additional libraries compared to the first tutorial.\nfrom urllib import request\nimport json\nimport requests\nimport sys\nimport shutil\nfrom urllib.parse import urlencode\n\n\n\nLet’s start up the client from the harmony-py library and define the CMR url.\n\nharmony_client = Client(env=Environment.PROD)\ncmr_root = 'cmr.earthdata.nasa.gov'\n\n\n\nKaRIn CalVal\n\nSearch by cycle and pass using CMR\nCMR Search: Number of item returned is limited to 10,000 (or 1 million if targeting collections) https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#paging-details\n\nprovider = 'POCLOUD'\nshort_name = 'SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1'\n\ntarget_cycle = list(range(1,101))\ntarget_pass = [2, 17]\npage_size = 2000\n\ngranuleIDs = []\nparams_without_cycle = [\n    ('page_size', page_size),\n    ('sort_key', \"-start_date\"),\n    ('provider', provider),\n    ('ShortName', short_name),\n    #('collection_concept_id', ccid), \n    #('token', token),\n    #('cycle', targetcycle), # can only specify one cycle at a time when specifying passes \n    ('passes[0][pass]', target_pass[0]), \n    ('passes[1][pass]', target_pass[1]),\n]\n\nfor v in target_cycle:\n    params = [(\"cycle[]\", v)]\n    params.extend(params_without_cycle)\n    # print(params)\n    query = urlencode(params)\n    cmr_url = f\"https://{cmr_root}/search/granules.umm_json?{query}\"\n    # print(cmr_url)\n    response = requests.get(cmr_url)\n    response_body = response.json()\n    \n    for itm in response_body['items']:\n        granuleIDs.append(itm['meta']['concept-id'])\n\nlen(granuleIDs) # Note the 200-granule limit\n\n\n\nThen perform a spatial subset and download using Harmony\nOn the back end, the subsetting part of Harmony is powered by L2SS-py\n\n# collection = Collection(id=ccid)\ncollection = Collection(id=short_name)\n\n# start_day = datetime(2015,4,15,0,0,0)\n# end_day = datetime(2015,4,17,0,0,0)\n\nrequest = Request(\n    collection=collection,\n    spatial=BBox(-140, 20, -100, 50), # [20-50N], [-140W, -100W] CA Current\n    #variables=[],\n    # temporal={\n    #     'start': start_day,\n    #     'stop': end_day # goal: try up to 21 days at least,\n    #},\n    granule_id=granuleIDs,\n)\n\nrequest.is_valid()\n\n\nprint(harmony_client.request_as_curl(request))\njob_id = harmony_client.submit(request)\nprint(f'Job ID: {job_id}')\n\n\nharmony_client.status(job_id)\n\n\n# results = harmony_client.result_urls(job_id, link_type=LinkType.s3)\n# urls = list(results)\n# print(urls)\n\n\n# create a new folder to put the subsetted data in\nos.makedirs(\"swot_ocean\",exist_ok = True)\n\n\nfutures = harmony_client.download_all(job_id, directory='./swot_ocean', overwrite=True)\nfile_names = [f.result() for f in futures]\nsorted(file_names)\n\n\nds = xr.open_mfdataset(sorted(file_names),combine='nested',concat_dim='num_lines')\nds\n\n\n# Plot only a pair of passes at a time\ni_time = np.arange(0,1725*2)+1725*90\n\nfig = plt.figure(figsize=[11,7]) \nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\nax.set_extent([-150, -90, 10, 60])\nplt.scatter(ds.longitude[i_time,:], ds.latitude[i_time,:], lw=1, c=ds.ssha_karin[i_time,:])\nplt.colorbar(label='SSHA (m)')\nplt.clim(-1,1)\nplt.show()"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#getting-started",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#getting-started",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Getting Started",
    "text": "Getting Started\nIn this notebook will show direct access of PO.DAAC archived products in the Earthdata Cloud in AWS Simple Storage Service (S3). In this demo, we will showcase the usage of SWOT Simulated Level-2 KaRIn SSH from GLORYS for Science Version 1. More information on the datasets can be found at https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF files into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\nIn the future, if you want to use this notebook as a reference, please note that we are not doing collection discovery here - we assume the collection of interest has been determined.\n\nRequirements\nThis can run in the Small openscapes instance, that is, it only needs 8GB of memory and ~2 CPU.\nIf you want to run this in your own AWS account, you can use a t2.large instance, which also has 2 CPU and 8GB memory. It’s improtant to note that all instances using direct S3 access to PO.DAAC or Earthdata data are required to run in us-west-2, or the Oregon region.\nThis instance will cost approximately $0.0832 per hour. The entire demo can run in considerably less time.\n\n\nImports\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\nboto3\ns3fs\nxarray\nmatplotlib\ncartopy"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#learning-objectives",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nimport needed libraries\nauthenticate for NASA Earthdata archive (Earthdata Login) (here this takes place as part of obtaining the AWS credentials step)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data by downloading directly into your cloud workspace from S3 within US-west 2 and operating on those files.\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\nNote: no files are being donwloaded off the cloud, rather, we are working with the data in the AWS cloud.\n\nimport boto3\nimport json\nimport xarray as xr\nimport s3fs\nimport os\nimport requests\nimport cartopy.crs as ccrs\nfrom matplotlib import pyplot as plt\nfrom os import path\n%matplotlib inline"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#get-a-temporary-aws-access-key-based-on-your-earthdata-login-user-id",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#get-a-temporary-aws-access-key-based-on-your-earthdata-login-user-id",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Get a temporary AWS Access Key based on your Earthdata Login user ID",
    "text": "Get a temporary AWS Access Key based on your Earthdata Login user ID\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects (i.e. data) from applicable Earthdata Cloud buckets (storage space). For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs.\nThe below methods (get_temp_dreds) requires the user to have a ‘netrc’ file in the users home directory.\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\nWe will now get a credential for the ‘PO.DAAC’ provider and set up our environment to use those values.\nNOTE if you see an error like ‘HTTP Basic: Access denied.’ It means the username/password you’ve entered is incorrect.\nNOTE2 If you get what looks like a long HTML page in your error message (e.g. \n\n…), the right netrc ‘machine’ might be missing.\n\ncreds = get_temp_creds('podaac')\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = creds[\"accessKeyId\"]\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = creds[\"secretAccessKey\"]\nos.environ[\"AWS_SESSION_TOKEN\"] = creds[\"sessionToken\"]\n\ns3 = s3fs.S3FileSystem(anon=False)"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#location-of-data-in-the-po.daac-s3-archive",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#location-of-data-in-the-po.daac-s3-archive",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Location of data in the PO.DAAC S3 Archive",
    "text": "Location of data in the PO.DAAC S3 Archive\nWe need to determine the path for our products of interest. We can do this through several mechanisms. Those are described in the Finding_collection_concept_ids.ipynb notebook, or the Pre-Workshop material, https://podaac.github.io/2022-SWOT-Ocean-Cloud-Workshop/prerequisites/01_Earthdata_Search.html.\nAfter using the Finding_collection_concept_ids.ipynb guide to find our S3 location, we end up with:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#now-that-we-have-the-s3-bucket-location-for-the-data-of-interest",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#now-that-we-have-the-s3-bucket-location-for-the-data-of-interest",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Now that we have the S3 bucket location for the data of interest…",
    "text": "Now that we have the S3 bucket location for the data of interest…\nIt’s time to find our data! Below we are using a glob to find file names matching a pattern. Here, we want any files matching the pattern used below; here this equates, in science, terms, to Cycle 001 and the first 10 passes. This information can be gleaned from product description documents. Another way of finding specific data files would be to search on cycle/pass from CMR or Earthdata Search GUI and use the S3 links provided in the resulting metadata or access links, respectively, directly instead of doing a glob (essentially an ‘ls’).\nThe files we are looking at are about 11-13 MB each. So the 10 we’re looking to access are about ~100 MB total.\n\ns3path = 's3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_00*.nc'\nremote_files = s3.glob(s3path)\n\n\nremote_files\n\n['podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_001_20140412T120000_20140412T125126_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_002_20140412T125126_20140412T134253_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_003_20140412T134253_20140412T143420_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_004_20140412T143420_20140412T152546_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_005_20140412T152547_20140412T161713_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_006_20140412T161714_20140412T170840_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_007_20140412T170840_20140412T180007_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_008_20140412T180008_20140412T185134_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_009_20140412T185134_20140412T194301_DG10_01.nc']"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#a-final-word",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#a-final-word",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "A final word…",
    "text": "A final word…\nAccessing data completely from S3 and in memory are affected by various things.\n\nThe format of the data - archive formats like NetCDF, GEOTIFF, HDF vs cloud optimized data structures (Zarr, kerchunk, COG). Cloud formats are made for accessing only the pieces of data of interest needed at the time of the request (e.g. a subset, timestep, etc).\nThe internal structure of the data. Tools like xarray make a lot of assumptions about how to open and read a file. Sometimes the internals don’t fit the xarray ‘mould’ and we need to continue to work with data providers and software providers to make these two sides work together. Level 2 data (non-gridded), specifically, suffers from some of the assumptions made."
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subset-swot-l2-ssh-using-dask",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subset-swot-l2-ssh-using-dask",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Subset SWOT L2 SSH using Dask",
    "text": "Subset SWOT L2 SSH using Dask\nJinbo Wang\nSWOT Scientist/PODAAC Project Scientist\n3/15/2022"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#simulated-l2-ssh",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#simulated-l2-ssh",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Simulated L2 SSH",
    "text": "Simulated L2 SSH\nThe SWOTsimulator was used on two global ocean simulations (LLC4320 and GLORYS) following the error specification described in Level 2 KaRIn Low Rate Sea Surface Height Product PDF file (D-56407). The simulator was based on Gaultier et al. first version but was almost completely rewritten, while keeping the error budget largely unchanged. (The error representation is claimed to be better but needs more documentation.) The CNES team is the creator of these products. The 8 datasets are listed as follows:\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1    10288 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1    10287 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nThe links to these datasets on PODAAC website or Earthdata search page.\n\nEight simulated L2 SSH datasets on PO.DAAC website.\nFour GLORYS datasets from Earthdata search\nFour ECCO_LLC4320 datasets from Earthdata search\n\nThe ECCO_LLC4320-based products cover one-year duration with >10k files (granules in Earthdata language) in each collection. The GLORYS-based products cover about 20 months with >17k files in each collection."
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#relevant-content-in-the-simulated-l2-ssh-files",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#relevant-content-in-the-simulated-l2-ssh-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Relevant content in the simulated L2 SSH files",
    "text": "Relevant content in the simulated L2 SSH files\nThere are too much information in these datasets for this short cloud hackathon. For example, there are 92 variables in the KaRIn products. The hackathon participants are strongly encouraged to read the the product description to understand the meaning of different variables. For this exercise, the variables with “simulated” keyword are most relevant: 1. ‘simulated_true_ssh_karin’ 1. ‘simulated_error_baseline_dilation’ 1. ‘simulated_error_roll’ 1. ‘simulated_error_phase’ 1. ‘simulated_error_timing’ 1. ‘simulated_error_karin’ 1. ‘simulated_error_orbital’ 1. ‘simulated_error_troposphere’"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subsetting-using-dask-delay",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subsetting-using-dask-delay",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Subsetting using Dask delay",
    "text": "Subsetting using Dask delay\nSteps to use Dask within jupyter-notebook 1. Go to the Dask extension (third button on the left-hand side vertical bar). 1. Start a Dask cluster by clicking “+” NEW. You will get the following: \n\nUse “SCALE” to maximumize the number of workers according to your AWS instance.\n\nCopy tcp://127.0.0.1:????? link from the LocalCluster and replace the following scheduler variable.\n\n\nscheduler='tcp://127.0.0.1:34721'\nout_folder='/home/jovyan/podaac_hackathon_swot_20220316/subsets/' #change this to your folder\n\n\n#The 8 datasets have short names starting from \"SWOT\". \n#You may get more than these 8 collections in the future after new SWOT data ingested. \n#The following wildcard will give you the 8 collections for now.\n#%%timeit\n\ns3sys=init_S3FileSystem()\n\ns3path=\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/*nc\"  \n#Search all collections that fit to the wildcard.\nfns= s3sys.glob(s3path)\nprint(len(fns))\n\nswot_karin_l2_subset_dask(scheduler,fns[:1000], \n                          'simulated_true_ssh_karin', \n                          [-126,-120,30,50], \n                           out_folder=out_folder )"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#print-the-content-of-an-example-file",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#print-the-content-of-an-example-file",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Print the content of an example file",
    "text": "Print the content of an example file\nLoad and plot one subset.\n\nfrom glob import glob\nfns_local=sorted(glob('subsets/*nc'))\n\ndata=xr.open_dataset(fns_local[10])\ndata\n\nLet us look at one error field “simulated_error_troposphere”.\n\nplt.scatter(data['longitude'],data['latitude'],s=1,c=data['simulated_true_ssh_karin'])\n\n\nPlease engage with PODAAC by sharing your experience, sending feedback, and asking questions, either through podaac@podaac.jpl.nasa.gov or any of the PODAAC team members who you feel comfortable to communicate. Engagement leads to better services. Go SWOT!"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#a-podaac-cloud-hackathon-for-swot-oceanography-science-teams",
    "href": "tutorials/SWOT_sim_data_intro.html#a-podaac-cloud-hackathon-for-swot-oceanography-science-teams",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "A PODAAC Cloud Hackathon for SWOT Oceanography Science Teams",
    "text": "A PODAAC Cloud Hackathon for SWOT Oceanography Science Teams\nJinbo Wang\nSWOT Scientist/PODAAC Project Scientist\n3/16/2022"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#swot-oceanography-datacode-survey-2021-science-team-meeting",
    "href": "tutorials/SWOT_sim_data_intro.html#swot-oceanography-datacode-survey-2021-science-team-meeting",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "SWOT Oceanography data/code survey (2021 Science Team Meeting)",
    "text": "SWOT Oceanography data/code survey (2021 Science Team Meeting)\nShortly before the 2021 SWOT Science Team meeting, a survey was created to ask for the SWOT oceanography community’s opinion about data and code sharing. In summary, the community showed high interests in a set of synthetic SWOT global L2 SSH hosted by PODAAC/AVISO, as well as pre-launch training. The survey response can be accessed here. The following are a few takeaways.\n\n\n\nimage.png\n\n\n\nL2 basic/expert SSH will be mostly used data products at the initial stage\nA common datesets will be very useful for preparation of the SWOT research\nIt is important for PODAAC/AVISO to host simulated datasets to create postlaunch scenarios\nMost teams need training in data access and analyses\nMajorities are potentially interested in co-developing softwares and toolboxes."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#simulated-l2-ssh",
    "href": "tutorials/SWOT_sim_data_intro.html#simulated-l2-ssh",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Simulated L2 SSH",
    "text": "Simulated L2 SSH\nThe SWOTsimulator was used on two global ocean simulations (LLC4320 and GLORYS) following the error specification described in Level 2 KaRIn Low Rate Sea Surface Height Product PDF file (D-56407). The simulator was based on Lucile/Clement/Fu’s first version but was almost completely rewritten, while keeping the error budget largely unchanged. (The error representation is claimed to be better but needs more documentation.) The CNES team is the creator of these products. The 8 datasets are listed as follows:\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1    10288 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1    10287 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nThe links to these datasets on PODAAC website or Earthdata search page.\n\nEight simulated L2 SSH datasets on PO.DAAC website.\nFour GLORYS datasets from Earthdata search\nFour ECCO_LLC4320 datasets from Earthdata search\n\nThe ECCO_LLC4320-based products cover one-year duration with >10k files (granules in Earthdata language) in each collection. The GLORYS-based products cover about 20 months with >17k files in each collection."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#relevant-content-in-the-simulated-l2-ssh-files",
    "href": "tutorials/SWOT_sim_data_intro.html#relevant-content-in-the-simulated-l2-ssh-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Relevant content in the simulated L2 SSH files",
    "text": "Relevant content in the simulated L2 SSH files\nThere are too much information in these datasets for this short cloud hackathon. For example, there are 92 variables in the KaRIn products. The hackathon participants are strongly encouraged to read the the product description to understand the meaning of different variables. For this exercise, the variables with “simulated” keyword are most relevant: 1. ‘simulated_true_ssh_karin’ 1. ‘simulated_error_baseline_dilation’ 1. ‘simulated_error_roll’ 1. ‘simulated_error_phase’ 1. ‘simulated_error_timing’ 1. ‘simulated_error_karin’ 1. ‘simulated_error_orbital’ 1. ‘simulated_error_troposphere’\nA visualized example is provided at the end of this notebook.\nNote that the LLC4320 SSH includes the barotropic (BT) signals, internal tides and atmospheric pressure (IB). A proper correction is not yet provided/validated. Wang et al. (2018) found that a linear detrend within ~150km range may be sufficient to remove most of the large-scale BT and IB signals. It can be a quick solution but no guarantee about the size of the residual errors."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#other-swot-relevant-datasets",
    "href": "tutorials/SWOT_sim_data_intro.html#other-swot-relevant-datasets",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Other SWOT relevant datasets",
    "text": "Other SWOT relevant datasets\nOther datasets in PODAAC that are relevant to SWOT include 10 regional subsets of LLC4320 with all the 2D and 3D fields created to support Adopt-A-Crossover (AdAC) project. You will find the dynamic correspondence of the ECCO_LLC4320-based L2 SSH to these subsets. Sentinel-6MF alongtrack data are also hosted in PODAAC cloud.\n\nAll LLC4320-derived datasets in PODAAC\nSentinel-6MF alongtracks"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#acknowledgment",
    "href": "tutorials/SWOT_sim_data_intro.html#acknowledgment",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Acknowledgment",
    "text": "Acknowledgment\n\nProject/HQ\n\nF. Briol, Gerald Dibarbource, Nicolas Picot, Shailen Desai produced the data products\nJ. Tom Farrar, Julien le Sommer, Ryan Abernathey, Sarah Gille, Rosemary Morrow, Lee-Lueng Fu provided science team support\nNadya Vinogradova requested this cloud hackathon. Jessica Hausman facilitated the coordination.\nJustin Rice facilitates the PODAAC-openscapes collaboration\n\nOpenscapes provides cloud-jupyterhub\nPO.DAAC team (everything else here)"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#browsing-the-8-datasets-in-s3podaac-ops-cumulus-protected",
    "href": "tutorials/SWOT_sim_data_intro.html#browsing-the-8-datasets-in-s3podaac-ops-cumulus-protected",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Browsing the 8 datasets in s3://podaac-ops-cumulus-protected",
    "text": "Browsing the 8 datasets in s3://podaac-ops-cumulus-protected\nAll PODAAC data collections are stored in s3://podaac-ops-cumulus-protected and/or s3://podaac-ops-cumulus-public. Once you are on AWS computing instance, they can be accessed like accessing to your own harddrives from your laptop.\n\n#The 8 datasets have short names starting from \"SWOT\". \n#You may get more than these 8 collections in the future after new SWOT data ingested. \n#The following wildcard will give you the 8 collections for now.\ns3path=\"s3://podaac-ops-cumulus-protected/SWOT*\"  \n\n#Search all collections that fit to the wildcard.\nfns= s3sys.glob(s3path)\n\n#Print the short names of the collection (also used as 'folder' names) and the number of files (granules) within. \nfor aa in fns:\n    print('%55s'%aa.split('/')[-1], len(s3sys.glob(aa+'/*nc')), 'files')\n\n#Print the name of the first 10 files in the SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1 collection.\nfns=s3sys.glob(\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/*nc\")\npprint(fns[:10])\n\n     SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1 10288 files\n    SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1 10218 files\n           SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1 17686 files\n          SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 17564 files\n     SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1 10287 files\n    SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1 10218 files\n           SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1 17686 files\n          SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1 17564 files\n['podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_001_20111113T000000_20111113T005105_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_002_20111113T005105_20111113T014211_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_003_20111113T014211_20111113T023317_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_004_20111113T023317_20111113T032423_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_005_20111113T032423_20111113T041529_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_006_20111113T041529_20111113T050634_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_007_20111113T050634_20111113T055739_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_008_20111113T055739_20111113T064845_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_009_20111113T064845_20111113T073951_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_010_20111113T073951_20111113T083057_DG10_01.nc']"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#print-the-content-of-an-example-file",
    "href": "tutorials/SWOT_sim_data_intro.html#print-the-content-of-an-example-file",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Print the content of an example file",
    "text": "Print the content of an example file\nThe following code uses the function temporarily defined in the first block to open a file from ‘fns’. The function includes establishing a direct S3 access and opening with xarray. It returns a xarray Dataset.\n\ndata=open_swot_L2SSH(fns[0])\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                                (num_lines: 9868, num_pixels: 71,\n                                            num_sides: 2)\nCoordinates:\n    latitude                               (num_lines, num_pixels) float64 ...\n    longitude                              (num_lines, num_pixels) float64 ...\n    latitude_nadir                         (num_lines) float64 -77.66 ... 77.66\n    longitude_nadir                        (num_lines) float64 144.8 ... 311.9\nDimensions without coordinates: num_lines, num_pixels, num_sides\nData variables: (12/92)\n    time                                   (num_lines) datetime64[ns] 2011-11...\n    time_tai                               (num_lines) datetime64[ns] 2011-11...\n    ssh_karin                              (num_lines, num_pixels) float64 ...\n    ssh_karin_uncert                       (num_lines, num_pixels) float32 ...\n    ssha_karin                             (num_lines, num_pixels) float64 ...\n    ssh_karin_2                            (num_lines, num_pixels) float64 ...\n    ...                                     ...\n    simulated_error_timing                 (num_lines, num_pixels) float64 ...\n    simulated_error_roll                   (num_lines, num_pixels) float64 ...\n    simulated_error_phase                  (num_lines, num_pixels) float64 ...\n    simulated_error_karin                  (num_lines, num_pixels) float64 ...\n    simulated_error_orbital                (num_lines, num_pixels) float64 ...\n    simulated_error_troposphere            (num_lines, num_pixels) float64 ...\nAttributes: (12/32)\n    Conventions:                CF-1.7\n    title:                      Level 2 Low Rate Sea Surface Height Data Prod...\n    institution:                CNES/JPL\n    source:                     Simulate product\n    history:                    2021-09-23 07:47:11Z : Creation\n    platform:                   SWOT\n    ...                         ...\n    right_last_longitude:       311.89957599684425\n    right_last_latitude:        77.03365811434979\n    wavelength:                 0.008385803020979\n    orbit_solution:             POE\n    ellipsoid_semi_major_axis:  6378137.0\n    ellipsoid_flattening:       0.0033528106647474805xarray.DatasetDimensions:num_lines: 9868num_pixels: 71num_sides: 2Coordinates: (4)latitude(num_lines, num_pixels)float64...long_name :latitude (positive N, negative S)standard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.[700628 values with dtype=float64]longitude(num_lines, num_pixels)float64...long_name :longitude (degrees East)standard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.[700628 values with dtype=float64]latitude_nadir(num_lines)float64...long_name :latitude of satellite nadir pointstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Geodetic latitude [-80,80] (degrees north of equator) of the satellite nadir point.array([-77.662495, -77.662454, -77.662388, ...,  77.663153,  77.663181,\n        77.663183])longitude_nadir(num_lines)float64...long_name :longitude of satellite nadir pointstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude (degrees east of Grenwich meridian) of the satellite nadir point.array([144.766698, 144.850876, 144.935054, ..., 311.732881, 311.817064,\n       311.901247])Data variables: (92)time(num_lines)datetime64[ns]...long_name :time in UTCstandard_name :timeleap_second :YYYY-MM-DDThh:mm:ssZcomment :Time of measurement in seconds in the UTC time scale since 1 Jan 2000 00:00:00 UTC. [tai_utc_difference] is the difference between TAI and UTC reference time (seconds) for the first measurement of the data set. If a leap second occurs within the data set, the attribute leap_second is set to the UTC time at which the leap second occurs.tai_utc_difference :34.0array(['2011-11-16T13:09:22.641982976', '2011-11-16T13:09:22.955580992',\n       '2011-11-16T13:09:23.269179968', ..., '2011-11-16T14:00:27.610507008',\n       '2011-11-16T14:00:27.922805952', '2011-11-16T14:00:28.235110016'],\n      dtype='datetime64[ns]')time_tai(num_lines)datetime64[ns]...long_name :time in TAIstandard_name :timetai_utc_difference :[Value of TAI-UTC at time of first record]comment :Time of measurement in seconds in the TAI time scale since 1 Jan 2000 00:00:00 TAI. This time scale contains no leap seconds. The difference (in seconds) with time in UTC is given by the attribute [time:tai_utc_difference].array(['2011-11-16T13:09:56.641982976', '2011-11-16T13:09:56.955580992',\n       '2011-11-16T13:09:57.269179968', ..., '2011-11-16T14:01:01.610507008',\n       '2011-11-16T14:01:01.922805952', '2011-11-16T14:01:02.235110016'],\n      dtype='datetime64[ns]')ssh_karin(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Fully corrected sea surface height measured by KaRIn. The height is relative to the reference ellipsoid defined in the global attributes. This value is computed using radiometer measurements for wet troposphere effects on the KaRIn measurement (e.g., rad_wet_tropo_cor and sea_state_bias_cor).[700628 values with dtype=float64]ssh_karin_uncert(num_lines, num_pixels)float32...long_name :sea surface height anomaly uncertaintyunits :mvalid_min :0valid_max :60000comment :1-sigma uncertainty on the sea surface height from the KaRIn measurement.[700628 values with dtype=float32]ssha_karin(num_lines, num_pixels)float64...long_name :sea surface height anomalyunits :mvalid_min :-1000000valid_max :1000000comment :Sea surface height anomaly from the KaRIn measurement = ssh_karin - mean_sea_surface_cnescls - solid_earth_tide - ocean_tide_fes – internal_tide_hret - pole_tide - dac.[700628 values with dtype=float64]ssh_karin_2(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Fully corrected sea surface height measured by KaRIn. The height is relative to the reference ellipsoid defined in the global attributes. This value is computed using model-based estimates for wet troposphere effects on the KaRIn measurement (e.g., model_wet_tropo_cor and sea_state_bias_cor_2).[700628 values with dtype=float64]ssha_karin_2(num_lines, num_pixels)float64...long_name :sea surface height anomalyunits :mvalid_min :-1000000valid_max :1000000comment :Sea surface height anomaly from the KaRIn measurement = ssh_karin_2 - mean_sea_surface_cnescls - solid_earth_tide - ocean_tide_fes – internal_tide_hret - pole_tide - dac.[700628 values with dtype=float64]ssha_karin_qual(num_lines, num_pixels)float64...long_name :sea surface height quality flagstandard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for the SSHA from KaRIn.[700628 values with dtype=float64]polarization_karin(num_lines, num_sides)object...long_name :polarization for each side of the KaRIn swathcomment :H denotes co-polarized linear horizontal, V denotes co-polarized linear vertical.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=object)swh_karin(num_lines, num_pixels)float32...long_name :significant wave height from KaRInstandard_name :sea_surface_wave_significant_heightunits :mvalid_min :0valid_max :25000comment :Significant wave height from KaRIn volumetric correlation.[700628 values with dtype=float32]swh_karin_uncert(num_lines, num_pixels)float32...long_name :1-sigma uncertainty on significant wave height from KaRInunits :mvalid_min :0valid_max :25000comment :1-sigma uncertainty on significant wave height from KaRIn.[700628 values with dtype=float32]sig0_karin(num_lines, num_pixels)float32...long_name :normalized radar cross section (sigma0) from KaRInstandard_name :surface_backwards_scattering_coefficient_of_radar_waveunits :1valid_min :-1000.0valid_max :10000000.0comment :Normalized radar cross section (sigma0) from KaRIn in real, linear units (not decibels).  The value may be negative due to noise subtraction.  The value is corrected for instrument calibration and atmospheric attenuation. Radiometer measurements provide the atmospheric attenuation (sig0_cor_atmos_rad).[700628 values with dtype=float32]sig0_karin_uncert(num_lines, num_pixels)float32...long_name :1-sigma uncertainty on sigma0 from KaRInunits :1valid_min :0.0valid_max :1000.0comment :1-sigma uncertainty on sigma0 from KaRIn.[700628 values with dtype=float32]sig0_karin_2(num_lines, num_pixels)float32...long_name :normalized radar cross section (sigma0) from KaRInstandard_name :surface_backwards_scattering_coefficient_of_radar_waveunits :1valid_min :-1000.0valid_max :10000000.0comment :Normalized radar cross section (sigma0) from KaRIn in real, linear units (not decibels).  The value may be negative due to noise subtraction.  The value is corrected for instrument calibration and atmospheric attenuation. A meteorological model provides the atmospheric attenuation (sig0_cor_atmos_model).[700628 values with dtype=float32]wind_speed_karin(num_lines, num_pixels)float32...long_name :wind speed from KaRIn standard_name :wind_speedsource :TBDunits :m/svalid_min :0valid_max :65000comment :Wind speed from KaRIn computed from sig0_karin.[700628 values with dtype=float32]wind_speed_karin_2(num_lines, num_pixels)float32...long_name :wind speed from KaRIn standard_name :wind_speedsource :TBDunits :m/svalid_min :0valid_max :65000comment :Wind speed from KaRIn computed from sig0_karin_2.[700628 values with dtype=float32]swh_karin_qual(num_lines, num_pixels)float64...long_name :quality flag for significant wave height from KaRIn.standard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for significant wave height from KaRIn.[700628 values with dtype=float64]sig0_karin_qual(num_lines, num_pixels)float64...long_name :quality flag for sigma0 from KaRIn.standard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for sigma0 from KaRIn.[700628 values with dtype=float64]num_pt_avg(num_lines, num_pixels)float32...long_name :number of samples averagedunits :1valid_min :0valid_max :289comment :Number of native unsmoothed, beam-combined KaRIn samples averaged.[700628 values with dtype=float32]swh_model(num_lines, num_pixels)float32...long_name :significant wave height from wave modelstandard_name :sea_surface_wave_significant_heightsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :0valid_max :30000comment :Significant wave height from model.[700628 values with dtype=float32]mean_wave_direction(num_lines, num_pixels)float32...long_name :mean sea surface wave directionsource :Meteo France Wave Model (MF-WAM)institution :Meteo Franceunits :degreevalid_min :0valid_max :36000comment :Mean sea surface wave direction.[700628 values with dtype=float32]mean_wave_period_t02(num_lines, num_pixels)float32...long_name :sea surface wind wave mean periodstandard_name :sea_surface_wave_significant_periodsource :Meteo France Wave Model (MF-WAM)institution :Meteo Franceunits :svalid_min :0valid_max :100comment :Sea surface wind wave mean period from model spectral density second moment.[700628 values with dtype=float32]wind_speed_model_u(num_lines, num_pixels)float32...long_name :u component of model windstandard_name :eastward_windsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :m/svalid_min :-30000valid_max :30000comment :Eastward component of the atmospheric model wind vector at 10 meters.[700628 values with dtype=float32]wind_speed_model_v(num_lines, num_pixels)float32...long_name :v component of model windstandard_name :northward_windsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :m/svalid_min :-30000valid_max :30000comment :Northward component of the atmospheric model wind vector at 10 meters.[700628 values with dtype=float32]wind_speed_rad(num_lines, num_sides)float32...long_name :wind speed from radiometerstandard_name :wind_speedsource :Advanced Microwave Radiometerunits :m/svalid_min :0valid_max :65000comment :Wind speed from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)distance_to_coast(num_lines, num_pixels)float32...long_name :distance to coastsource :MODIS/GlobCoverinstitution :European Space Agencyunits :mvalid_min :0valid_max :21000comment :Approximate distance to the nearest coast point along the Earth surface.[700628 values with dtype=float32]heading_to_coast(num_lines, num_pixels)float32...long_name :heading to coastunits :degreesvalid_min :0valid_max :35999comment :Approximate compass heading (0-360 degrees with respect to true north) to the nearest coast point.[700628 values with dtype=float32]ancillary_surface_classification_flag(num_lines, num_pixels)float32...long_name :surface classificationstandard_name :status_flagsource :MODIS/GlobCoverinstitution :European Space Agencyflag_meanings :open_ocean land continental_water aquatic_vegetation continental_ice_snow floating_ice salted_basinflag_values :[0 1 2 3 4 5 6]valid_min :0valid_max :6comment :7-state surface type classification computed from a mask built with MODIS and GlobCover data.[700628 values with dtype=float32]dynamic_ice_flag(num_lines, num_pixels)float32...long_name :dynamic ice flagstandard_name :status_flagsource :EUMETSAT Ocean and Sea Ice Satellite Applications Facilityinstitution :EUMETSATflag_meanings :no_ice probable_ice iceflag_values :[0 1 2]valid_min :0valid_max :2comment :Dynamic ice flag for the location of the KaRIn measurement.[700628 values with dtype=float32]rain_flag(num_lines, num_pixels)float32...long_name :rain flagstandard_name :status_flagflag_meanings :no_rain probable_rain rainflag_values :[0 1 2]valid_min :0valid_max :2comment :Flag indicates that signal is attenuated, probably from rain.[700628 values with dtype=float32]rad_surface_type_flag(num_lines, num_sides)float32...long_name :radiometer surface type flagstandard_name :status_flagsource :Advanced Microwave Radiometerflag_meanings :open_ocean coastal_ocean landflag_values :[0 1 2]valid_min :0valid_max :2comment :Flag indicating the validity and type of processing applied to generate the wet troposphere correction (rad_wet_tropo_cor). A value of 0 indicates that open ocean processing is used, a value of 1 indicates coastal processing, and a value of 2 indicates that rad_wet_tropo_cor is invalid due to land contamination.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)sc_altitude(num_lines)float64...long_name :altitude of KMSF originstandard_name :height_above_reference_ellipsoidunits :mvalid_min :0valid_max :2000000000comment :Altitude of the KMSF origin.array([nan, nan, nan, ..., nan, nan, nan])orbit_alt_rate(num_lines)float32...long_name :orbital altitude rate with respect to mean sea surfaceunits :m/svalid_min :-3500valid_max :3500comment :Orbital altitude rate with respect to the mean sea surface.array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)cross_track_angle(num_lines)float64...long_name :cross-track angle from true northunits :degreesvalid_min :0valid_max :359999999comment :Angle with respect to true north of the cross-track direction to the right of the spacecraft velocity vector.array([nan, nan, nan, ..., nan, nan, nan])sc_roll(num_lines)float64...long_name :roll of the spacecraftstandard_name :platform_roll_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude roll angle; positive values move the +y antenna down.array([nan, nan, nan, ..., nan, nan, nan])sc_pitch(num_lines)float64...long_name :pitch of the spacecraftstandard_name :platform_pitch_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude pitch angle; positive values move the KMSF +x axis up.array([nan, nan, nan, ..., nan, nan, nan])sc_yaw(num_lines)float64...long_name :yaw of the spacecraftstandard_name :platform_yaw_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude yaw angle relative to the nadir track. The yaw angle is a right-handed rotation about the nadir (downward) direction. A yaw value of 0 deg indicates that the KMSF +x axis is aligned with the horizontal component of the Earth-relative velocity vector. A yaw value of 180 deg indicates that the spacecraft is in a yaw-flipped state, with the KMSF -x axis aligned with the horizontal component of the Earth-relative velocity vector.array([nan, nan, nan, ..., nan, nan, nan])velocity_heading(num_lines)float64...long_name :heading of the spacecraft Earth-relative velocity vectorunits :degreesvalid_min :0valid_max :359999999comment :Angle with respect to true north of the horizontal component of the spacecraft Earth-relative velocity vector. A value of 90 deg indicates that the spacecraft velocity vector pointed due east. Values between 0 and 90 deg indicate that the velocity vector has a northward component, and values between 90 and 180 deg indicate that the velocity vector has a southward component.array([nan, nan, nan, ..., nan, nan, nan])orbit_qual(num_lines)float32...long_name :orbit quality flag standard_name :status_flagvalid_min :0valid_max :1comment :Orbit quality flag.array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)latitude_avg_ssh(num_lines, num_pixels)float64...long_name :weighted average latitude of samples used to compute SSHstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.  This value may be biased away from a nominal grid location if some of the native, unsmoothed samples were discarded during processing.[700628 values with dtype=float64]longitude_avg_ssh(num_lines, num_pixels)float64...long_name :weighted average longitude of samples used to compute SSHstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.  This value may be biased away from a nominal grid location if some of the native, unsmoothed samples were discarded during processing.[700628 values with dtype=float64]cross_track_distance(num_lines, num_pixels)float32...long_name :cross track distanceunits :mvalid_min :-75000.0valid_max :75000.0comment :Distance of sample from nadir. Negative values indicate the left side of the swath, and positive values indicate the right side of the swath.[700628 values with dtype=float32]x_factor(num_lines, num_pixels)float32...long_name :radiometric calibration X factor as a composite value for the X factors of the +y and -y channelsunits :1valid_min :0.0valid_max :1e+20comment :Radiometric calibration X factor as a linear power ratio.[700628 values with dtype=float32]sig0_cor_atmos_model(num_lines, num_pixels)float32...long_name :two-way atmospheric correction to sigma0 from modelsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :1valid_min :1.0valid_max :10.0comment :Atmospheric correction to sigma0 from weather model data as a linear power multiplier (not decibels). sig0_cor_atmos_model is already applied in computing sig0_karin_2.[700628 values with dtype=float32]sig0_cor_atmos_rad(num_lines, num_pixels)float32...long_name :two-way atmospheric correction to sigma0 from radiometer datasource :Advanced Microwave Radiometerunits :1valid_min :1.0valid_max :10.0comment :Atmospheric correction to sigma0 from radiometer data as a linear power multiplier (not decibels). sig0_cor_atmos_rad is already applied in computing sig0_karin.[700628 values with dtype=float32]doppler_centroid(num_lines, num_sides)float32...long_name :doppler centroid estimated by KaRInunits :1/svalid_min :-30000valid_max :30000comment :Doppler centroid (in hertz or cycles per second) estimated by KaRIn.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)phase_bias_ref_surface(num_lines, num_pixels)float64...long_name :height of reference surface used for phase bias calculationunits :mvalid_min :-15000000valid_max :150000000comment :Height (relative to the reference ellipsoid) of the reference surface used for phase bias calculation during L1B processing.[700628 values with dtype=float64]obp_ref_surface(num_lines, num_pixels)float64...long_name :height of reference surface used by on-board-processorunits :mvalid_min :-15000000valid_max :150000000comment :Height (relative to the reference ellipsoid) of the reference surface used by the KaRIn on-board processor.[700628 values with dtype=float64]rad_tmb_187(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 18.7 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :13000valid_max :25000comment :Main beam brightness temperature measurement at 18.7 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_tmb_238(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 23.8 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :13000valid_max :25000comment :Main beam brightness temperature measurement at 23.8 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_tmb_340(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 34.0 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :15000valid_max :28000comment :Main beam brightness temperature measurement at 34.0 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_water_vapor(num_lines, num_sides)float32...long_name :water vapor content from radiometerstandard_name :atmosphere_water_vapor_content source :Advanced Microwave Radiometerunits :kg/m^2valid_min :0valid_max :15000comment :Integrated water vapor content from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_cloud_liquid_water(num_lines, num_sides)float32...long_name :liquid water content from radiometerstandard_name :atmosphere_cloud_liquid_water_content  source :Advanced Microwave Radiometerunits :kg/m^2valid_min :0valid_max :2000comment :Integrated cloud liquid water content from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)mean_sea_surface_cnescls(num_lines, num_pixels)float64...long_name :mean sea surface height (CNES/CLS)source :CNES_CLS_15institution :CNES/CLSunits :mvalid_min :-1500000valid_max :1500000comment :Mean sea surface height above the reference ellipsoid. The value is referenced to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_sea_surface_cnescls_uncert(num_lines, num_pixels)float32...long_name :mean sea surface height accuracy (CNES/CLS)source :CNES_CLS_15institution :CNES/CLSunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean sea surface height (mean_sea_surface_cnescls).[700628 values with dtype=float32]mean_sea_surface_dtu(num_lines, num_pixels)float64...long_name :mean sea surface height (DTU)source :DTU18institution :DTUunits :mvalid_min :-1500000valid_max :1500000comment :Mean sea surface height above the reference ellipsoid. The value is referenced to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_sea_surface_dtu_uncert(num_lines, num_pixels)float32...long_name :mean sea surface height accuracy (DTU)source :DTU18institution :DTUunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean sea surface height (mean_sea_surface_dtu)[700628 values with dtype=float32]geoid(num_lines, num_pixels)float64...long_name :geoid heightstandard_name :geoid_height_above_reference_ellipsoidsource :EGM2008 (Pavlis et al., 2012)units :mvalid_min :-1500000valid_max :1500000comment :Geoid height above the reference ellipsoid with a correction to refer the value to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_dynamic_topography(num_lines, num_pixels)float32...long_name :mean dynamic topographysource :CNES_CLS_18institution :CNES/CLSunits :mvalid_min :-30000valid_max :30000comment :Mean dynamic topography above the geoid.[700628 values with dtype=float32]mean_dynamic_topography_uncert(num_lines, num_pixels)float32...long_name :mean dynamic topography accuracysource :CNES_CLS_18institution :CNES/CLSunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean dynamic topography.[700628 values with dtype=float32]depth_or_elevation(num_lines, num_pixels)float32...long_name :ocean depth or land elevationsource :Altimeter Corrected Elevations, version 2institution :European Space Agencyunits :mvalid_min :-12000valid_max :10000comment :Ocean depth or land elevation above reference ellipsoid. Ocean depth (bathymetry) is given as negative values, and land elevation positive values.[700628 values with dtype=float32]solid_earth_tide(num_lines, num_pixels)float32...long_name :solid Earth tide heightsource :Cartwright and Taylor (1971) and Cartwright and Edden (1973)units :mvalid_min :-10000valid_max :10000comment :Solid-Earth (body) tide height. The zero-frequency permanent tide component is not included.[700628 values with dtype=float32]ocean_tide_fes(num_lines, num_pixels)float64...long_name :geocentric ocean tide height (FES)source :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-300000valid_max :300000comment :Geocentric ocean tide height. Includes the sum total of the ocean tide, the corresponding load tide (load_tide_fes) and equilibrium long-period ocean tide height (ocean_tide_eq).[700628 values with dtype=float64]ocean_tide_got(num_lines, num_pixels)float64...long_name :geocentric ocean tide height (GOT)source :GOT4.10c (Ray, 2013)institution :GSFCunits :mvalid_min :-300000valid_max :300000comment :Geocentric ocean tide height. Includes the sum total of the ocean tide, the corresponding load tide (load_tide_got) and equilibrium long-period ocean tide height (ocean_tide_eq).[700628 values with dtype=float64]load_tide_fes(num_lines, num_pixels)float32...long_name :geocentric load tide height (FES)source :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-2000valid_max :2000comment :Geocentric load tide height. The effect of the ocean tide loading of the Earth's crust. This value has already been added to the corresponding ocean tide height value (ocean_tide_fes).[700628 values with dtype=float32]load_tide_got(num_lines, num_pixels)float32...long_name :geocentric load tide height (GOT)source :GOT4.10c (Ray, 2013)institution :GSFCunits :mvalid_min :-2000valid_max :2000comment :Geocentric load tide height. The effect of the ocean tide loading of the Earth's crust. This value has already been added to the corresponding ocean tide height value (ocean_tide_got).[700628 values with dtype=float32]ocean_tide_eq(num_lines, num_pixels)float32...long_name :equilibrium long-period ocean tide heightunits :mvalid_min :-2000valid_max :2000comment :Equilibrium long-period ocean tide height. This value has already been added to the corresponding ocean tide height values (ocean_tide_fes and ocean_tide_got).[700628 values with dtype=float32]ocean_tide_non_eq(num_lines, num_pixels)float32...long_name :non-equilibrium long-period ocean tide heightsource :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-2000valid_max :2000comment :Non-equilibrium long-period ocean tide height. This value is reported as a relative displacement with repsect to ocean_tide_eq. This value can be added to ocean_tide_eq, ocean_tide_fes, or ocean_tide_got, or subtracted from ssha_karin and ssha_karin_2, to account for the total long-period ocean tides from equilibrium and non-equilibrium contributions.[700628 values with dtype=float32]internal_tide_hret(num_lines, num_pixels)float32...long_name :coherent internal tide (HRET)source :Zaron (2019)units :mvalid_min :-2000valid_max :2000comment :Coherent internal ocean tide. This value is subtracted from the ssh_karin and ssh_karin_2 to compute ssha_karin and ssha_karin_2, respectively.[700628 values with dtype=float32]internal_tide_sol2(num_lines, num_pixels)float32...long_name :coherent internal tide (Model 2)source :TBDunits :mvalid_min :-2000valid_max :2000comment :Coherent internal tide.[700628 values with dtype=float32]pole_tide(num_lines, num_pixels)float32...long_name :geocentric pole tide heightsource :Wahr (1985) and Desai et al. (2015)units :mvalid_min :-2000valid_max :2000comment :Geocentric pole tide height.  The total of the contribution from the solid-Earth (body) pole tide height, the ocean pole tide height, and the load pole tide height (i.e., the effect of the ocean pole tide loading of the Earth's crust).[700628 values with dtype=float32]dac(num_lines, num_pixels)float32...long_name :dynamic atmospheric correctionsource :MOG2Dinstitution :LEGOS/CNES/CLSunits :mvalid_min :-12000valid_max :12000comment :Model estimate of the effect on sea surface topography due to high frequency air pressure and wind effects and the low-frequency height from inverted barometer effect (inv_bar_cor). This value is subtracted from the ssh_karin and ssh_karin_2 to compute ssha_karin and ssha_karin_2, respectively. Use only one of inv_bar_cor and dac.[700628 values with dtype=float32]inv_bar_cor(num_lines, num_pixels)float32...long_name :static inverse barometer effect on sea surface heightunits :mvalid_min :-2000valid_max :2000comment :Estimate of static effect of atmospheric pressure on sea surface height. Above average pressure lowers sea surface height. Computed by interpolating ECMWF pressure fields in space and time. The value is included in dac. To apply, add dac to ssha_karin and ssha_karin_2 and subtract inv_bar_cor.[700628 values with dtype=float32]model_dry_tropo_cor(num_lines, num_pixels)float32...long_name :dry troposphere vertical correctionsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :-30000valid_max :-15000comment :Equivalent vertical correction due to dry troposphere delay. The reported sea surface height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height.[700628 values with dtype=float32]model_wet_tropo_cor(num_lines, num_pixels)float32...long_name :wet troposphere vertical correction from weather model datasource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :-10000valid_max :0comment :Equivalent vertical correction due to wet troposphere delay from weather model data. The reported pixel height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height (ssh_karin_2) results in the uncorrected sea surface height.[700628 values with dtype=float32]rad_wet_tropo_cor(num_lines, num_pixels)float32...long_name :wet troposphere vertical correction from radiometer datasource :Advanced Microwave Radiometerunits :mvalid_min :-10000valid_max :0comment :Equivalent vertical correction due to wet troposphere delay from radiometer measurements. The reported pixel height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height (ssh_karin) results in the uncorrected sea surface height.[700628 values with dtype=float32]iono_cor_gim_ka(num_lines, num_pixels)float32...long_name :ionosphere vertical correctionsource :Global Ionosphere Mapsinstitution :JPLunits :mvalid_min :-5000valid_max :0comment :Equivalent vertical correction due to ionosphere delay. The reported sea surface height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height.[700628 values with dtype=float32]height_cor_xover(num_lines, num_pixels)float64...long_name :height correction from KaRIn crossoversunits :mvalid_min :-100000valid_max :100000comment :Height correction from KaRIn crossover calibration. To apply this correction the value of height_cor_xover should be added to the value of ssh_karin, ssh_karin_2, ssha_karin, and ssha_karin_2.[700628 values with dtype=float64]correction_flag(num_lines, num_pixels)float32...long_name :quality flag for correctionsstandard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for corrections.[700628 values with dtype=float32]rain_rate(num_lines, num_pixels)float32...long_name :rain rate from weather modelsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mm/hrvalid_min :0valid_max :200comment :Rain rate from weather model.[700628 values with dtype=float32]ice_conc(num_lines, num_pixels)float32...long_name :concentration of sea icestandard_name :sea_ice_area_fractionsource :EUMETSAT Ocean and Sea Ice Satellite Applications Facilityinstitution :EUMETSATunits :%valid_min :0valid_max :10000comment :Concentration of sea ice from model.[700628 values with dtype=float32]sea_state_bias_cor(num_lines, num_pixels)float32...long_name :sea state bias correction to heightsource :TBDunits :mvalid_min :-6000valid_max :0comment :Sea state bias correction to ssh_karin. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height. The wind_speed_karin value is used to compute this quantity.[700628 values with dtype=float32]sea_state_bias_cor_2(num_lines, num_pixels)float32...long_name :sea state bias correction to heightsource :TBDunits :mvalid_min :-6000valid_max :0comment :Sea state bias correction to ssh_karin_2. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height. The wind_speed_karin_2 value is used to compute this quantity.[700628 values with dtype=float32]swh_sea_state_bias(num_lines, num_pixels)float32...long_name :SWH used in sea state bias correctionunits :mvalid_min :0valid_max :25000comment :Significant wave height used in sea state bias correction.[700628 values with dtype=float32]simulated_true_ssh_karin(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Height of the sea surface free of measurement errors.[700628 values with dtype=float64]simulated_error_baseline_dilation(num_lines, num_pixels)float64...long_name :Error due to baseline mast dilationunits :m[700628 values with dtype=float64]simulated_error_timing(num_lines, num_pixels)float64...long_name :Timing errorunits :m[700628 values with dtype=float64]simulated_error_roll(num_lines, num_pixels)float64...long_name :Error due to rollunits :m[700628 values with dtype=float64]simulated_error_phase(num_lines, num_pixels)float64...long_name :Error due to phaseunits :m[700628 values with dtype=float64]simulated_error_karin(num_lines, num_pixels)float64...long_name :KaRIn errorunits :m[700628 values with dtype=float64]simulated_error_orbital(num_lines, num_pixels)float64...long_name :Error due to orbital perturbationsunits :m[700628 values with dtype=float64]simulated_error_troposphere(num_lines, num_pixels)float64...long_name :Error due to wet troposphere path delayunits :m[700628 values with dtype=float64]Attributes: (32)Conventions :CF-1.7title :Level 2 Low Rate Sea Surface Height Data Product - Expert SSH with Wind and Waveinstitution :CNES/JPLsource :Simulate producthistory :2021-09-23 07:47:11Z : Creationplatform :SWOTproduct_version :1.2.1.dev10references :Gaultier, L., C. Ubelmann, and L.-L. Fu, 2016: The Challenge of Using Future SWOT Data for Oceanic Field Reconstruction. J. Atmos. Oceanic Technol., 33, 119-126, doi:10.1175/jtech-d-15-0160.1. http://dx.doi.org/10.1175/JTECH-D-15-0160.1.reference_document :D-56407_SWOT_Product_Description_L2_LR_SSHcontact :CNES aviso@altimetry.fr, JPL podaac@podaac.jpl.nasa.govcycle_number :4pass_number :17equator_time :2011-11-16T13:34:56.899302Zequator_longitude :228.16567754818402time_coverage_start :2011-11-16T13:09:22.641983Ztime_coverage_end :2011-11-16T14:00:28.235110Zgeospatial_lon_min :144.76234679489446geospatial_lon_max :311.9031061982219geospatial_lat_min :-78.2920195134443geospatial_lat_max :78.29270807524945left_first_longitude :144.76234679489446left_first_latitude :-77.03297084205963left_last_longitude :311.9031061982219left_last_latitude :78.29270807524945right_first_longitude :144.7714992348211right_first_latitude :-78.2920195134443right_last_longitude :311.89957599684425right_last_latitude :77.03365811434979wavelength :0.008385803020979orbit_solution :POEellipsoid_semi_major_axis :6378137.0ellipsoid_flattening :0.0033528106647474805\n\n\nLet us look at one error field “simulated_error_troposphere”.\n\ndata['simulated_error_troposphere']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'simulated_error_troposphere' (num_lines: 9868, num_pixels: 71)>\n[700628 values with dtype=float64]\nCoordinates:\n    latitude         (num_lines, num_pixels) float64 ...\n    longitude        (num_lines, num_pixels) float64 ...\n    latitude_nadir   (num_lines) float64 -77.66 -77.66 -77.66 ... 77.66 77.66\n    longitude_nadir  (num_lines) float64 144.8 144.9 144.9 ... 311.7 311.8 311.9\nDimensions without coordinates: num_lines, num_pixels\nAttributes:\n    long_name:  Error due to wet troposphere path delay\n    units:      mxarray.DataArray'simulated_error_troposphere'num_lines: 9868num_pixels: 71...[700628 values with dtype=float64]Coordinates: (4)latitude(num_lines, num_pixels)float64...long_name :latitude (positive N, negative S)standard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.[700628 values with dtype=float64]longitude(num_lines, num_pixels)float64...long_name :longitude (degrees East)standard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.[700628 values with dtype=float64]latitude_nadir(num_lines)float64-77.66 -77.66 ... 77.66 77.66long_name :latitude of satellite nadir pointstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Geodetic latitude [-80,80] (degrees north of equator) of the satellite nadir point.array([-77.662495, -77.662454, -77.662388, ...,  77.663153,  77.663181,\n        77.663183])longitude_nadir(num_lines)float64144.8 144.9 144.9 ... 311.8 311.9long_name :longitude of satellite nadir pointstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude (degrees east of Grenwich meridian) of the satellite nadir point.array([144.766698, 144.850876, 144.935054, ..., 311.732881, 311.817064,\n       311.901247])Attributes: (2)long_name :Error due to wet troposphere path delayunits :m\n\n\nThe following line returns a list of filenames with the full S3 link within the collection SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1, the simulated KaRIn L2 swaths on the science orbit based on ECCO_LLC4320.\n\nfns=s3sys.glob(\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/*nc\")\n\nPlot eight simulated L2 fields including the model truth and simulated errors.\n\nThere may be a bug in creating the timing error by the new simulator. According to SWOT_D-79084, the timing drift error, to the first order, creates a constant height bias across the swath. It accounts for 10% of the overall error budget. The first version of the SWOTsimulator correctly implemented it but not in the second simulator. Currently I am communicting with the CNES team.\n\n\ndata=open_swot_L2SSH(fns[100])\nfig,ax=plt.subplots(2,4,figsize=(20,10),sharex=True)\n\naxx=ax.flatten()\n\nkeys=[]\nfor key in data.keys():\n    if 'simulated' in key:\n        keys.append(key)\nprint(keys)\nfor i, key in enumerate(keys):\n    data[key][1000:1100,:].plot(ax=axx[i],)\n    axx[i].set_title(key)\nplt.tight_layout()\n\n['simulated_true_ssh_karin', 'simulated_error_baseline_dilation', 'simulated_error_roll', 'simulated_error_phase', 'simulated_error_timing', 'simulated_error_karin', 'simulated_error_orbital', 'simulated_error_troposphere']\n\n\n\n\n\n\n#Plot an example of one pass\n\nimport os\nos.getcwd()\nimport pylab as plt\nimport numpy as np\n\ndd=s3sys.open(fns[1000])\nds=xr.open_dataset(dd)\n\n#print(ds.keys())\n\n#ds=nc.Dataset(\"SWOT_L2_LR_SSH_Expert_018_290_20121112T003212_20121112T012339_DG10_01.nc\")\n#print(ds.variables.keys())\n\nsla=ds['simulated_true_ssh_karin'].data.flatten()\nlat=ds['latitude'].data.flatten()\nlon=ds['longitude'].data.flatten()\n\n#mask=(lat>-40)&(lat<-20)&(lon>90)&(lon<96)\n#sla=np.ma.masked_array(sla,mask=~mask)\n#sla=np.ma.masked_invalid(sla)\n#print(sla.mean())\n#sla=sla-sla.mean()\n\nplt.figure(figsize=(10,6))\nplt.scatter(lon,lat,c=sla,cmap=plt.cm.jet,s=1)\nplt.colorbar(shrink=0.5)\n#plt.xlim(90,96)\n#plt.ylim(-40,-20)\n\n<matplotlib.colorbar.Colorbar at 0x7fb08f856af0>\n\n\n\n\n\n\nSave all passes into a signal file (experimental)\nI used the following to save the lat/lon/time information from all passes and into a single (big) file. You can use it as a template to save other variables.\n%%timeit\n#KaRIn LLC4320 Science orbit s3path=“s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_*nc”\n#s3path=“podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/*nc”\nfns= s3sys.glob(s3path) print(len(fns),fns[0])\ndd=s3sys.open(fns[0]) ds=xr.open_dataset(dd) msk=~np.isnan(ds[‘simulated_error_baseline_dilation’].data[0,:]) lon=ds[‘longitude’][:,msk].values.astype(‘>f4’) lat=ds[‘latitude’][:,msk].values.astype(‘>f4’) time=ds[‘time’].values\ndel dd, ds\nlons=xr.DataArray(lon,name=‘Longitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lats=xr.DataArray(lat,name=‘Latitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time})\nfor fn in fns[1:]: #print(fn) dd=s3sys.open(fn) ds=xr.open_dataset(dd) lon=ds[‘longitude’][:,msk].values.astype(‘>f4’) lat=ds[‘latitude’][:,msk].values.astype(‘>f4’) time=ds[‘time’].values lon=xr.DataArray(lon,name=‘Longitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lat=xr.DataArray(lat,name=‘Latitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lons=xr.concat([lons,lon],dim=‘time’) lats=xr.concat([lats,lat],dim=‘time’) del lon,lat, dd, ds, time\nxr.Dataset({‘Longitude’:lons,‘Latitude’:lats}).to_netcdf(‘SWOT_KaRIn_CALVAL_LatLon.nc’)\n\nPlease engage with PODAAC by sharing your experience, sending feedback, and asking questions, either through podaac@podaac.jpl.nasa.gov or any of the PODAAC team members who you feel comfortable to communicate. Engagement leads to better services. Go SWOT!"
  },
  {
    "objectID": "tutorials/02_Subset_SWOT_sim.html#temporal-and-spatial-subset-using-a-bounding-box-around-ca-crossover-region",
    "href": "tutorials/02_Subset_SWOT_sim.html#temporal-and-spatial-subset-using-a-bounding-box-around-ca-crossover-region",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Temporal and spatial subset using a bounding box around CA Crossover region",
    "text": "Temporal and spatial subset using a bounding box around CA Crossover region\n\nDefine the collection of interest by calling Collection(id = YourCollection), where YourCollection is a collection short name or concept-id. There are a number of ways to get the collection shortname; using Earthdata Search is one way - see pre-workshop tutorial.\nSet time bounds.\nSet spatial bounding box.\nThere are also other options such as variables, granules, and concatenation.\n\n\ncollection = Collection(id='SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1')\n\nstart_day = datetime(2015,4,15,0,0,0)\nend_day = datetime(2015,4,20,0,0,0)\n\nrequest = Request(\n    collection=collection,\n    temporal={\n        'start': start_day,\n        'stop': end_day\n    },\n    spatial=BBox(-140, 20, -100, 50), # [20-50N], [140W-100W] CA Current crossover point (35N,125W)\n    # variables=[],\n    # granule_id=granuleIDs,\n    # concatenate = True,\n)\n\nrequest.is_valid()\n\n\nprint(harmony_client.request_as_curl(request))\njob_id = harmony_client.submit(request)\nprint(f'Job ID: {job_id}') # This job id is shareable:show how to do this\n\n\nView the job status\nA Harmony request is limited to 200 granules. The limit is there to prevent users from accidentally make huge requests.\n\nharmony_client.status(job_id) \n\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n\n\nDownload subsetted files\nFilenames that end with “subsetted.nc4” have been subsetted.\nThe other filenames (that are un-altered) indicate that these were rounded up as relevant files during Harmony’s search, but do not contain data in the actual region of interest, so the files downloaded here are empty.\n\n# create a new folder to put the subsetted data in\nos.makedirs(\"swot_ocean_basic_subset\",exist_ok = True)\n\n\nfutures = harmony_client.download_all(job_id, directory='./swot_ocean_basic_subset', overwrite=True)\nfile_names = [f.result() for f in futures]\nsorted(file_names)\n\n\nfrom os import listdir\nfrom os.path import isfile, join\ndata_files = [ f for f in file_names if \"subsetted\" in f]\ndata_files\n\n\n\nTake a look at the subset data\nNote: xarray is a little clunky with variables in groups.\n\nds = xr.open_mfdataset(sorted(data_files),combine='nested',concat_dim='time',group='data_01')\nds\n\n\nfig = plt.figure(figsize=[11,7]) \nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\nax.set_extent([-150, -90, 10, 60])\nplt.scatter(ds.longitude, ds.latitude, lw=1, c=ds.depth_or_elevation)\nplt.colorbar(label='Depth or elevation (m)')\nplt.clim(-4000,4000)\nplt.show()\n# ds.plot.scatter( y=\"latitude\",\n#                  x=\"longitude\", \n#                  hue=\"depth_or_elevation\",\n#                  s=1,\n#                  vmin=-4000,\n#                  vmax=4000,\n#                  levels=9, \n#                  cmap=\"jet\",\n#                  aspect=2.5,\n#                  size=9, )\n\n\n\nOn your own: Try subsetting by variable or granule IDs, or over a longer time period"
  },
  {
    "objectID": "tutorials/Finding_collection_s3_location.html#finding-s3-location-information-from-the-po.daac-portal",
    "href": "tutorials/Finding_collection_s3_location.html#finding-s3-location-information-from-the-po.daac-portal",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Finding S3 Location information from the PO.DAAC Portal",
    "text": "Finding S3 Location information from the PO.DAAC Portal\nThe easiest of which is through the PO.DAAC Cloud Dataset Listing page: https://podaac.jpl.nasa.gov/cloud-datasets\n\n\n\nS3 Data Locations from Portal\n\n\nFor each dataset, the ‘Data Access’ tab will have various information, but will always contain the S3 paths listed specifically. Data files will always be found under the ‘protected’ bucket."
  },
  {
    "objectID": "tutorials/Finding_collection_s3_location.html#finding-s3-location-from-earthdata-search",
    "href": "tutorials/Finding_collection_s3_location.html#finding-s3-location-from-earthdata-search",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Finding S3 Location from Earthdata Search",
    "text": "Finding S3 Location from Earthdata Search\nFrom the Earthdata Search Client (search.earthdata.nasa.gov), collection level information can be found by clicking the ‘i’ on a collection search result. An example of this is seen below:\n\n\n\nS3 Data Locations from Search 1\n\n\nOnce on the collection inforamtion screen, the S3 bucket locations can be found by scrolling to the bottom of the information panel. The SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 example is shown below.\n\n\n\nS3 Data Locations from Search 2"
  },
  {
    "objectID": "tutorials/Finding_collection_s3_location.html#finding-s3-location-from-cmr",
    "href": "tutorials/Finding_collection_s3_location.html#finding-s3-location-from-cmr",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Finding S3 Location from CMR",
    "text": "Finding S3 Location from CMR\nOne can query the collection identifier to get information from CMR:\nhttps://cmr.earthdata.nasa.gov/search/concepts/C2152045877-POCLOUD.umm_json\nThe identifier is found on the PO.DAAC Cloud Data Set Listing page entries, called ‘Collection Concept ID’\nResults returned will look like the following:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "href": "tutorials/On-prem_Cloud_example.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud",
    "text": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#summary",
    "href": "tutorials/On-prem_Cloud_example.html#summary",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Summary",
    "text": "Summary\n\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics."
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#requirements",
    "href": "tutorials/On-prem_Cloud_example.html#requirements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#learning-objectives",
    "href": "tutorials/On-prem_Cloud_example.html#learning-objectives",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#import-packages",
    "href": "tutorials/On-prem_Cloud_example.html#import-packages",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/On-prem_Cloud_example.html#specify-data-time-range-and-area-of-interest",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project)."
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/On-prem_Cloud_example.html#search-and-download-icesat-2-atl07-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Sun, 12 Dec 2021 01:52:31 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/On-prem_Cloud_example.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/On-prem_Cloud_example.html#load-data-into-xarray-via-s3-direct-access",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fb3944adb50>\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fb39fcd8040>"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "href": "tutorials/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#what-is-xarray",
    "href": "tutorials/Intro_xarray_hvplot.html#what-is-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 2920)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "href": "tutorials/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (lat: 25, lon: 53)>\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 4)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)>\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)>\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920)>\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#analysis",
    "href": "tutorials/Intro_xarray_hvplot.html#analysis",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.air.resample(time='M').mean().hvplot()\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#plot-results",
    "href": "tutorials/Intro_xarray_hvplot.html#plot-results",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "tutorials/Earthdata_search.html",
    "href": "tutorials/Earthdata_search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECOSTRESS LSTE, hosted by the LP DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left.\n\n\n\nFigure caption: Search for ECCO data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECCO monthly SSH in the search box (which will produce 39 matching collections), and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nFigure caption: Refine search, set temporal bounds, get more information\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\nthe S3 storage bucket and object prefix where this data is located\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nPro Tip: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (here again it’s the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n4.a. Entire file content\nLet’s stay we are interested in the entire file content, so we select the “Direct Download” option (as opposed to other options to subset or transform the data):\n\n\n\nFigure caption: Customize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n  \nThe Download Files tab provides the https:// links for downloading the files locally. E.g.: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud (an example will be shown in Tutorial 3). E.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\nTip: Another quicker way to find the bucket and object prefix is from the list of data files the search returns. Next to the + green button is a grey donwload symbol. Click on that to see the Download Files https:// links or on the AWS S3 Access to get the direct S3:// access links, which contain the bucket and object prefix where data is stored.\n\n\n4.b. Subset or transform before download or access\nDAAC tools and services are also being migrated or developed in the cloud, next to that data. These include the Harmony API and OPeNDAP in the cloud, as a few examples.\nWe can leverage these cloud-based services on cloud-archived data to reduce or transform the data (depending on need) before getting the access links regardless of whether we prefer to download the data and work on a local machine or whether we want to access the data in the cloud (from a cloud workspace). These can be useful data reduction services that support a faster time to science.\nHarmony\nHarmony allows you to seamlessly analyze Earth observation data from different NASA data centers. These services (API endpoints) provide data reduction (e.g. subsetting) and transfromation services (e.g. convert netCDF data to Zarr cloud optimized format).\n\n\n\nFigure caption: Leverage Harmony cloud-based data transformation services\n\n\nWhen you click the final green Download button, the links provided are to data that had been transformed based on our selections on the previous screen (here chosing to use the Harmony service to reformat the data to Zarr). These data are staged for us in an S3 bucket in AWS, and we can use the s3:// links to access those specific data. This service also provides STAC access links. This particular example is applicable if your workflow is in the AWS us-west-2 region.\n\n\n\nFigure caption: Harmony-staged data in S3\n\n\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nIn tutorial 3 Direct Data Access, we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\nTutorial 3 will pick up from here and cover these next steps in more detail."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#timing",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#timing",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Timing",
    "text": "Timing\n\nExercise: 30 min"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#summary",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Explored available NASA Providers",
    "text": "Explored available NASA Providers\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nConnect to the CMR-STAC API\n\nprovider_cat = Client.open(STAC_URL)\n\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n\nproviders = [p for p in provider_cat.get_children()]\n\nfor count, provider in enumerate(providers):\n    print(f'{count} - {provider.title}')\n\n0 - LARC_ASDC\n1 - USGS_EROS\n2 - ESA\n3 - GHRC\n4 - LAADS\n5 - OBPG\n6 - OB_DAAC\n7 - ECHO\n8 - ISRO\n9 - LPCUMULUS\n10 - EDF_DEV04\n11 - GES_DISC\n12 - ASF\n13 - OMINRT\n14 - EUMETSAT\n15 - NCCS\n16 - NSIDCV0\n17 - PODAAC\n18 - LARC\n19 - USGS\n20 - SCIOPS\n21 - LANCEMODIS\n22 - CDDIS\n23 - JAXA\n24 - AU_AADC\n25 - ECHO10_OPS\n26 - LPDAAC_ECS\n27 - NSIDC_ECS\n28 - ORNL_DAAC\n29 - LM_FIRMS\n30 - SEDAC\n31 - LANCEAMSR2\n32 - NOAA_NCEI\n33 - USGS_LTA\n34 - GESDISCCLD\n35 - GHRSSTCWIC\n36 - ASIPS\n37 - ESDIS\n38 - POCLOUD\n39 - NSIDC_CPRD\n40 - ORNL_CLOUD\n41 - FEDEO\n42 - XYZ_PROV\n43 - GHRC_DAAC\n44 - CSDA\n45 - NRSCC\n46 - CEOS_EXTRA\n47 - MOPITT\n48 - GHRC_CLOUD\n49 - LPCLOUD\n50 - CCMEO"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Connect to the LPCLOUD Provider/STAC Catalog",
    "text": "Connect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "List STAC Collections",
    "text": "List STAC Collections\nWe’ll create a products variable to view deeper in the STAC Catalog.\n\nproducts = [c for c in catalog.get_children()]\n\n\nPrint one of the STAC Collection records\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n\nproducts[1].to_dict()\n\n{'type': 'Collection',\n 'id': 'HLSL30.v2.0',\n 'stac_version': '1.0.0',\n 'description': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.',\n 'links': [{'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items',\n   'type': 'application/json',\n   'title': 'Granules in this collection'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html',\n   'type': 'text/html',\n   'title': 'HTML metadata for collection'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json',\n   'type': 'application/json',\n   'title': 'CMR JSON metadata for collection'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2020',\n   'type': 'application/json',\n   'title': '2020 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2021',\n   'type': 'application/json',\n   'title': '2021 catalog'},\n  {'rel': <RelType.SELF: 'self'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': <RelType.PARENT: 'parent'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>}],\n 'stac_extensions': [],\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2013-05-01T00:00:00Z', None]]}},\n 'license': 'not-provided'}\n\n\n\n\nPrint the STAC Collection ids with their title\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\n\nfor p in products: \n    print(f\"{p.id}: {p.title}\")\n\nASTGTM.v003: ASTER Global Digital Elevation Model V003\nHLSL30.v2.0: HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\nHLSL30.v1.5: HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5\nHLSS30.v1.5: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5\nHLSS30.v2.0: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API",
    "text": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. The geojson file is found in the ~/data directory. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the geojson file: “../tutorials/data/ne_w_agfields.geojson”\n\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfield\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POLYGON ((-101.67272 41.04754, -101.65345 41.0...\n    \n  \n\n\n\n\n\n\nVisualize contents of geojson file\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n\nfieldShape = field['geometry'][0]\nfieldShape\n\n\n\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\nExtract the coordinates for the ROI\n\nroi = json.loads(field.to_json())['features'][0]['geometry']\nroi\n\n{'type': 'Polygon',\n 'coordinates': [[[-101.67271614074707, 41.04754380304359],\n   [-101.65344715118408, 41.04754380304359],\n   [-101.65344715118408, 41.06213891056728],\n   [-101.67271614074707, 41.06213891056728],\n   [-101.67271614074707, 41.04754380304359]]]}\n\n\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\nSpecify date range\nNext up is to specify our date range using ISO_8601 date formatting.\n\n#date_range = \"2021-05-01T00:00:00Z/2021-08-30T23:59:59Z\"    # closed interval\n#date_range = \"2021-05-01T00:00:00Z/..\"                      # open interval - does not currently work with the CMR-STAC API\ndate_range = \"2021-05/2021-08\"\n\n\n\nSpecify the STAC Collections\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\ncollections\n\n['HLSL30.v2.0', 'HLSS30.v2.0']"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package.\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nPrint out how many STAC Items match our search query\n\nsearch.matched()\n\n113\n\n\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n\nitem_collection = search.get_all_items()\n\nLet’s list some of the Items from our pystac item_collection:\n\nlist(item_collection)[0:5]\n\n[<Item id=HLS.L30.T13TGF.2021124T173013.v2.0>,\n <Item id=HLS.L30.T14TKL.2021124T173013.v2.0>,\n <Item id=HLS.S30.T14TKL.2021125T172901.v2.0>,\n <Item id=HLS.S30.T13TGF.2021125T172901.v2.0>,\n <Item id=HLS.S30.T14TKL.2021128T173901.v2.0>]\n\n\nWe can view a single Item as a dictionary, as we did above with STAC Collections/products.\n\nitem_collection[0].to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n\ncloudcover = 25\n\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nIn this hypothetical workflow, we’ll extract the bands needed to calculate an enhanced vegetation index (EVI). Thus, the band needed include red, near infrared (NIR), and blue. We’ll also extract a quality band (i.e., Fmask) that we’d eventually use to perform per-pixel quality filtering.\nNotice that the band ids are in some case not one-to-one between the S30 and the L30 product. This is evident in the NIR band for each product where S30’s NIR band id is B8A and L30’s is B05. Note, the S30 product has an additional NIR band with a band id of B08, but the spectral ranges between B8A and B05 are more closely aligned. Visit the HLS Overview page to learn more about HLS spectral bands.\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\nevi_band_links[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']\n\n\nNOTE that HLS data is mapped to the Universal Transverse Mercator (UTM) projection and is tiled using the Sentinel-2 Military Grid Reference System (MGRS) UTM grid. Notice that in the list of links we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. In this case, these two tiles represent neighboring UTM zones. The tiles can be discern from the file name, which is the last element in a link (far right) following the last forward slash (/) - e.g., HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif. The figure below explains where to find the tile/UTM zone from the file name.\n\nWe will now split the list of links into separate logical sub-lists."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Split Data Links List into Logical Groupings",
    "text": "Split Data Links List into Logical Groupings\nWe have a list of links to data assets that meet our search and filtering criteria. Below we’ll split our list from above into lists first by tile/UTM zone and then further by individual bands bands. The commands that follow will do the splitting with python routines.\n\nSplit by UTM tile specified in the file name (e.g., T14TKL & T13TGF)\n\ntile_dicts = defaultdict(list)    # https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\n\n\nfor l in evi_band_links:\n    tile = l.split('.')[-6]\n    tile_dicts[tile].append(l)\n\n\nPrint dictionary keys and values, i.e. the data links\n\ntile_dicts.keys()\n\ndict_keys(['T13TGF', 'T14TKL'])\n\n\n\ntile_dicts['T13TGF'][:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B8A.tif']\n\n\nNow we will create a separate list of data links for each tile\n\ntile_links_T14TKL = tile_dicts['T14TKL']\ntile_links_T13TGF = tile_dicts['T13TGF']\n\n\n\nPrint band/layer links for HLS tile T13TGF\n\n# tile_links_T13TGF[:10]\n\n\n\n\nSplit the links by band\n\nbands_dicts = defaultdict(list)\n\n\nfor b in tile_links_T13TGF:\n    band = b.split('.')[-2]\n    bands_dicts[band].append(b)\n\n\nbands_dicts.keys()\n\ndict_keys(['B04', 'B05', 'Fmask', 'B02', 'B8A'])\n\n\n\nbands_dicts['B04']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021140T173021.v2.0/HLS.L30.T13TGF.2021140T173021.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021140T172859.v2.0/HLS.S30.T13TGF.2021140T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021145T172901.v2.0/HLS.S30.T13TGF.2021145T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021155T172901.v2.0/HLS.S30.T13TGF.2021155T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021156T173029.v2.0/HLS.L30.T13TGF.2021156T173029.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021158T173901.v2.0/HLS.S30.T13TGF.2021158T173901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021163T173909.v2.0/HLS.S30.T13TGF.2021163T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021165T172422.v2.0/HLS.L30.T13TGF.2021165T172422.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021165T172901.v2.0/HLS.S30.T13TGF.2021165T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021173T173909.v2.0/HLS.S30.T13TGF.2021173T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021185T172901.v2.0/HLS.S30.T13TGF.2021185T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021188T173037.v2.0/HLS.L30.T13TGF.2021188T173037.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021190T172859.v2.0/HLS.S30.T13TGF.2021190T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021193T173909.v2.0/HLS.S30.T13TGF.2021193T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021198T173911.v2.0/HLS.S30.T13TGF.2021198T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021200T172859.v2.0/HLS.S30.T13TGF.2021200T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021203T173909.v2.0/HLS.S30.T13TGF.2021203T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021204T173042.v2.0/HLS.L30.T13TGF.2021204T173042.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021208T173911.v2.0/HLS.S30.T13TGF.2021208T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021210T172859.v2.0/HLS.S30.T13TGF.2021210T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021215T172901.v2.0/HLS.S30.T13TGF.2021215T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021218T173911.v2.0/HLS.S30.T13TGF.2021218T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021220T173049.v2.0/HLS.L30.T13TGF.2021220T173049.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021220T172859.v2.0/HLS.S30.T13TGF.2021220T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021223T173909.v2.0/HLS.S30.T13TGF.2021223T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021228T173911.v2.0/HLS.S30.T13TGF.2021228T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021229T172441.v2.0/HLS.L30.T13TGF.2021229T172441.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021230T172859.v2.0/HLS.S30.T13TGF.2021230T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021235T172901.v2.0/HLS.S30.T13TGF.2021235T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021243T173859.v2.0/HLS.S30.T13TGF.2021243T173859.v2.0.B04.tif']"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Save links to a text file",
    "text": "Save links to a text file\nTo complete this exercise, we will save the individual link lists as separate text files with descriptive names.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to write to the data directory: “../tutorials/data/{name}”\n\nWrite links from CMR-STAC API to a file\n\nfor k, v in bands_dicts.items():\n    name = (f'HTTPS_T13TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            f.write(f\"{l}\" + '\\n')\n\n\n\nWrite links to file for S3 access\n\nfor k, v in bands_dicts.items():\n    name = (f'S3_T13TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            s3l = l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n            f.write(f\"{s3l}\" + '\\n')"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR-STAC_API.html#resources",
    "href": "tutorials/Data_Discovery_CMR-STAC_API.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\n\nSTAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nPySTAC Client Documentation\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API.html#what-is-cmr",
    "href": "tutorials/Data_Discovery_CMR_API.html#what-is-cmr",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "href": "tutorials/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API.html#how-to-search-cmr-from-python",
    "href": "tutorials/Data_Discovery_CMR_API.html#how-to-search-cmr-from-python",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "How to search CMR from Python",
    "text": "How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\n\nimport requests\nfrom pprint import pprint\n\nThen we need to authenticate with EarthData Login. Since we’ve already set this up in the previous lesson, here you need to enter your username before executing the cell.\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll build this url as a python variable.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud_hosted parameter that is either True if that collection is in the cloud and False if it is not. The migration of NASA data to the cloud is a work in progress. Not all collections tagged as cloud_hosted have granules. To search for only cloud_hosted datasets with granules, I also set has_granules to True.\nI also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case the response variable.\n\nresponse\n\n<Response [200]>\n\n\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n\nresponse.status_code\n\n200\n\n\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches and run requests.get again. Don’t forget to rerun the cell that assigns the url variable\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below. TODO: maybe some context for where the 2 elements k, v, come from?\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 3820\nConnection: keep-alive\nDate: Tue, 02 Nov 2021 22:07:37 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,14000.0,\"SENTINEL-1A_RAW\",\"1\",1214470561,1320]\nCMR-Hits: 917\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 435\nX-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 3f7e5e686bf8f19b9c786efbe99c7589.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: DEN52-C1\nX-Amz-Cf-Id: BXEUlb5ygB9nj9ygUbSZvIc3BE4k0d1Mn1qDd66IkkuX1rC_Z-mN6Q==\n\n\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n\nresponse.headers['CMR-Hits']\n\n'917'\n\n\nand\n\nresponse.headers['cmr-hits']\n\n'917'\n\n\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. To do this, I set the provider parameter to ASF.\nYou can modify the code below to explore all of the cloud-hosted datasets or cloud-hosted datasets from other providers. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWhen search by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs.\n\nprovider = 'ASF'\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\n\nresponse.headers['cmr-hits']\n\n'45'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2021-11-02T22:41:34.322Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=ASF\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:39.000Z\",\"dataset_id\":\"SENTINEL-1A_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470488-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:22.000Z\",\"dataset_id\":\"SENTINEL-1B_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985661-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:56.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470533-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:47.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985645-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:49.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985660-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:58.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214471521-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:20.000Z\",\"dataset_id\":\"SENTINEL-1A_RAW\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_RAW\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_RAW\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A level zero product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470561-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:15.000Z\",\"dataset_id\":\"SENTINEL-1A_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1A_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470496-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:02.000Z\",\"dataset_id\":\"SENTINEL-1B_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1B_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985617-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:26.000Z\",\"dataset_id\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Single-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470682-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]}]}}'\n\n\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]. A reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'ASF',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'ASF',\n 'dataset_id': 'SENTINEL-1A_SLC',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C1214470488-ASF',\n 'links': [{'href': 'https://vertex.daac.asf.alaska.edu/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['ASF', 'ESA/CS1CGS'],\n 'original_format': 'ECHO10',\n 'platforms': ['Sentinel-1A'],\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'SENTINEL-1A_SLC',\n 'summary': 'Sentinel-1A slant-range product',\n 'time_start': '2014-04-03T00:00:00.000Z',\n 'title': 'SENTINEL-1A_SLC',\n 'updated': '2021-07-15T19:16:39.000Z',\n 'version_id': '1'}\n\n\nThe first response is not the result I am looking for TODO: because xyz…but it does show a few variables that we can use to further refine the search. So I want to print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable. TODO: is it worth saying something about what “feed” and “entry” are?\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\n\n\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49. Then, we can re-run our for loop for the collections.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'provider': provider,\n                            'page_size': 49,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\nASF SENTINEL-1A_OCN C1214472977-ASF\nASF SENTINEL-1B_RAW C1327985647-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_HIGH_RES C1214470576-ASF\nASF SENTINEL-1B_OCN C1327985579-ASF\nASF SENTINEL-1A_METADATA_RAW C1214470532-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_HIGH_RES C1327985741-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms (BETA) C1595422627-ASF\nASF ALOS_AVNIR_OBS_ORI C1808440897-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_MEDIUM_RES C1214472994-ASF\nASF SENTINEL-1B_METADATA_RAW C1327985650-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_HIGH_RES C1327985571-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_MEDIUM_RES C1327985740-ASF\nASF SENTINEL-1A_METADATA_OCN C1266376001-ASF\nASF SENTINEL-1B_METADATA_OCN C1327985646-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES C1214472336-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_HIGH_RES C1214470732-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1214473170-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_MEDIUM_RES C1327985578-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_HIGH_RES C1327985619-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1327985739-ASF\nASF STS-68_BROWSE_GRD C1661710593-ASF\nASF STS-68_BROWSE_SLC C1661710596-ASF\nASF STS-59_BROWSE_GRD C1661710578-ASF\nASF STS-59_BROWSE_SLC C1661710581-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_FULL_RES C1214471197-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_FULL_RES C1214471960-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_FULL_RES C1214472978-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_FULL_RES C1214473165-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_FULL_RES C1327985697-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_FULL_RES C1327985651-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_FULL_RES C1327985644-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_FULL_RES C1327985674-ASF\nAlaska Satellite Facility Sentinel-1 Unwrapped Interferogram and Coherence Map (BETA) C1379535600-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Amplitude (BETA) C1596065640-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Coherence (BETA) C1596065639-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Connected Components (BETA) C1596065641-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Unwrapped Phase (BETA) C1595765183-ASF\nASF STS-59_GRD C1661710583-ASF\nASF STS-59_METADATA_GRD C1661710586-ASF"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API.html#granule-search",
    "href": "tutorials/Data_Discovery_CMR_API.html#granule-search",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Granule Search",
    "text": "Granule Search\nIn NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1214472336-ASF',\n                            'temporal': '2020-10-17T00:00:00Z,2020-10-18T23:59:59Z',\n                            'bounding_box': '76.08166,-67.1746,88.19689,21.04862',\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n6\n\n\n\ngranules = response.json()['feed']['entry']\n#for granule in granules:\n#    print(f'{granule[\"archive_center\"]} {granule[\"dataset_id\"]} {granule[\"id\"]}')\n\n\npprint(granules)\n\n[{'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954601581-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34836'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.163563 87.942726 -60.893669 89.293564 -59.279579 '\n                '96.119583 -57.619923 94.49958 -59.163563 87.942726']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E',\n  'time_end': '2020-10-17T13:20:39.000Z',\n  'time_start': '2020-10-17T13:20:09.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E-METADATA_GRD_MD',\n  'updated': '2020-10-19T17:13:39.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616816-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-66.104271 69.819366 -69.571243 74.741966 -67.42112 83.209152 '\n                '-64.210938 77.59269 -66.104271 69.819366']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B',\n  'time_end': '2020-10-17T14:57:20.000Z',\n  'time_start': '2020-10-17T14:56:16.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:35:34.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616638-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-62.765087 66.277176 -66.103951 69.81897 -64.211227 77.590164 '\n                '-61.060097 73.427185 -62.765087 66.277176']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE',\n  'time_end': '2020-10-17T14:58:20.000Z',\n  'time_start': '2020-10-17T14:57:20.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:33:31.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954805829-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-65.236397 83.18071 -68.712318 87.887711 -66.630493 96.161102 '\n                '-63.400326 90.779785 -65.236397 83.18071']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659',\n  'time_end': '2020-10-18T14:00:00.000Z',\n  'time_start': '2020-10-18T13:58:56.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659-METADATA_GRD_MD',\n  'updated': '2020-10-20T07:16:54.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954799806-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-61.876564 79.848251 -65.236069 83.180344 -63.400402 90.778 '\n                '-60.217258 86.840034 -61.876564 79.848251']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777',\n  'time_end': '2020-10-18T14:01:00.000Z',\n  'time_start': '2020-10-18T14:00:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:51:16.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954798927-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.113174 77.623962 -61.876228 79.847961 -60.217464 '\n                '86.837784 -57.570774 84.174744 -59.113174 77.623962']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D',\n  'time_end': '2020-10-18T14:01:48.000Z',\n  'time_start': '2020-10-18T14:01:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:46:01.000Z'}]"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\n\nSee how my Desktop is setup\n\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "href": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\n\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "href": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book\nSlack"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2022-ECOSTRESS-Cloud-Workshop is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "href": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "href": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Cloud Optimized GeoTIFF (COG)",
    "text": "Cloud Optimized GeoTIFF (COG)\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\nImport Packages\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "HTTPS Data Access",
    "text": "HTTPS Data Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "from pystac_client import Client\nimport stackstac\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "href": "how-tos/data_access/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "href": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "href": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Open",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Open\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "href": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Authentication\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "href": "how-tos/data_access/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "tutorials/Comparing TNC Tree Data to ECOSTRESS.html",
    "href": "tutorials/Comparing TNC Tree Data to ECOSTRESS.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "import xarray as xr\nimport rioxarray\nimport rasterio as rio\nimport os\nimport holoviews as hv\nimport hvplot.xarray\nimport hvplot.pandas\nimport geopandas as gpd\nimport shapely\nfrom shapely.geometry import box\nfrom shapely.geometry import Point\nfrom pyproj import Transformer\nfrom matplotlib.colors import ListedColormap\n#import rasterstats\nimport numpy as np\nimport seaborn as sns\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom os.path import basename\nimport xarray\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.pyplot as plt\n\n\nFIG_WIDTH_PX = 1080\nFIG_HEIGHT_PX = 720\nFIG_WIDTH_IN = 16\nFIG_HEIGHT_IN = 9\n\nLet’s use rioxarray to open the surface temperature layer from the L2T_LSTE product on the 10SGD tile covering the Dangermond Preserve and take a first look at this image using hvplot.\n\nET_filename = \"/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L3T_ET_PT-JPL_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L3T_ET_PT-JPL_12653_004_10SGD_20200928T224329_0700_01_ETdaily.tif\"\nET_image = rioxarray.open_rasterio(ET_filename, chunks='auto').squeeze('band', drop=True)\nET_image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 1568, x: 1568)>\ndask.array<getitem, shape=(1568, 1568), dtype=float32, chunksize=(1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7.001e+05 7.001e+05 ... 8.096e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    nan\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayy: 1568x: 1568dask.array<chunksize=(1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1568, 1568) \n                         (1568, 1568) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n\n        \n    \nCoordinates: (3)x(x)float647e+05 7.001e+05 ... 8.097e+05array([699995., 700065., 700135., ..., 809545., 809615., 809685.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 10Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]GeoTransform :699960.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (3)_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nThe ECOSTRESS Collection 2 tiled products are gridded in local UTM, following Sentinel convention, and this gridding is sampled at 70 m.\n\ncrs = ET_image.rio.crs\nprint(f\"CRS: {crs}\")\ncell_width, cell_height = ET_image.rio.resolution()\nprint(f\"resolution: {cell_width} m\")\n\nCRS: EPSG:32610\nresolution: 70.0 m\n\n\n\ncentroid_UTM = Point(np.nanmean(ET_image.x), np.nanmean(ET_image.y))\ncentroid_latlon = shapely.ops.transform(Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True).transform, centroid_UTM)\ncentroid_latlon.wkt\n\n'POINT (-120.2172393005597 34.71640207843576)'\n\n\nThe date for this ECOSTRESS scene is September 28th, 2020. This overpass was around 2:45 in the afternoon.\n\ndt = datetime.strptime(basename(ET_filename).split(\"_\")[-4], \"%Y%m%dT%H%M%S\")\ndt_solar = dt + timedelta(hours=(np.radians(centroid_latlon.x) / np.pi * 12))\nprint(f\"date/time UTC: {dt:%Y-%m-%d %H:%M:%S}\")\nprint(f\"date/time solar: {dt_solar:%Y-%m-%d %H:%M:%S}\")\n\ndate/time UTC: 2020-09-28 22:43:29\ndate/time solar: 2020-09-28 14:42:36\n\n\nTo plot this image on top of a basemap, we’ll reproject it on the fly to match the basemap.\n\nET_CMAP = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99974a\",\n    \"#53792d\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\n\nET_map = ET_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ET_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ET_image.quantile(0.02), ET_image.quantile(0.98)),\n    title=basename(ET_filename)\n)\n\nET_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nST_filename = \"/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L2T_LSTE_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L2T_LSTE_12653_004_10SGD_20200928T224329_0700_01_LST.tif\"\nST_image = rioxarray.open_rasterio(ST_filename).squeeze(\"band\", drop=True)\n\n\nST_image.hvplot(x='x', y='y') + ET_image.hvplot(x='x', y='y')\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nST_CMAP = \"jet\"\n\nattrs = ST_image.attrs\nST_image -= 273.15\nST_image.attrs = attrs\n\nST_map = ST_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ST_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_image.quantile(0.02), ST_image.quantile(0.98)),\n    title=basename(ST_filename)\n)\n\nST_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nSM_CMAP = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99894a\",\n    \"#2d6779\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\n\nSM_filename = \"Fall 2020/ECOv002_L3T_SM_PT-JPL_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L3T_SM_PT-JPL_12653_004_10SGD_20200928T224329_0700_01_SM.tif\"\nSM_image = rioxarray.open_rasterio(SM_filename).squeeze(\"band\", drop=True)\n\nSM_map = SM_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=SM_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(SM_image.quantile(0.02), SM_image.quantile(0.98)),\n    title=basename(SM_filename)\n)\n\nSM_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nNDVI_CMAP = [\n    \"#0000ff\",\n    \"#000000\",\n    \"#745d1a\",\n    \"#e1dea2\",\n    \"#45ff01\",\n    \"#325e32\"\n]\n\nNDVI_filename = \"Fall 2020/ECOv002_L2T_STARS_12653_004_10SGD_20200928_0700_01/ECOv002_L2T_STARS_12653_004_10SGD_20200928_0700_01_NDVI.tif\"\nNDVI_image = rioxarray.open_rasterio(NDVI_filename).squeeze(\"band\", drop=True)\n\nNDVI_map = NDVI_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=NDVI_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(NDVI_image.quantile(0.02), NDVI_image.quantile(0.98)),\n    title=basename(NDVI_filename)\n)\n\nNDVI_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nLet’s open our ground observations provided by the Nature Conservancy. TNC recorded observations of tree health for each tree and the location of the tree in latitude and longitude. These are qualitative categories of good, poor, and dead.\n\nTNC_fall_2020 = gpd.read_file(\"TNC_fall_2020.geojson\")\nprint(TNC_fall_2020.crs)\nTNC_fall_2020.head()\n\nepsg:4326\n\n\n\n\n\n  \n    \n      \n      planting_date\n      health\n      geometry\n    \n  \n  \n    \n      0\n      2019-05-10\n      good\n      POINT Z (-120.41578 34.52600 191.30000)\n    \n    \n      1\n      2019-05-10\n      good\n      POINT Z (-120.41590 34.52611 194.80000)\n    \n    \n      2\n      2019-05-10\n      poor\n      POINT Z (-120.41589 34.52611 194.20000)\n    \n    \n      3\n      2019-05-10\n      good\n      POINT Z (-120.41591 34.52618 192.80000)\n    \n    \n      4\n      2019-05-10\n      good\n      POINT Z (-120.41600 34.52621 188.90000)\n    \n  \n\n\n\n\nTo compare our in situ point data with the projected raster data, we need to project these coordinates into the local UTM projection of the raster image.\n\nTNC_fall_2020 = TNC_fall_2020.to_crs(ET_image.rio.crs)\nprint(TNC_fall_2020.crs)\nTNC_fall_2020.head()\n\nPROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]\n\n\n\n\n\n  \n    \n      \n      planting_date\n      health\n      geometry\n    \n  \n  \n    \n      0\n      2019-05-10\n      good\n      POINT Z (737195.535 3823512.913 191.300)\n    \n    \n      1\n      2019-05-10\n      good\n      POINT Z (737183.595 3823525.373 194.800)\n    \n    \n      2\n      2019-05-10\n      poor\n      POINT Z (737184.237 3823525.408 194.200)\n    \n    \n      3\n      2019-05-10\n      good\n      POINT Z (737182.283 3823532.980 192.800)\n    \n    \n      4\n      2019-05-10\n      good\n      POINT Z (737174.601 3823536.391 188.900)\n    \n  \n\n\n\n\nLet’s map out the in situ data. We’ll again reproject these points on the fly to match the basemap. Let’s assign colors to these categories as well, which we’ll use throughout the notebook.\n\n# is there a way to get the point colors in a legend?\ntree_palette = {\n    \"dead\": \"black\",\n    \"poor\": \"red\",\n    \"other\": \"white\",\n    \"good\": \"green\"\n}\n\nTNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    tiles=\"OSM\", \n    size=1.5, \n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX,\n    title=\"The Nature Conservancy Fall 2020 Tree Survey\"\n)\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nIn this projected space, let’s get the bounds of our study area in meters from the convex hull of our observation locations with a 100 meter buffer.\n\nxmin, ymin, xmax, ymax = TNC_fall_2020.unary_union.convex_hull.buffer(100).bounds\nxmin, ymin, xmax, ymax\n\n(733744.5626735839, 3821633.1073998627, 738346.4301366343, 3824320.7421952724)\n\n\nLet’s look at the tree health points overlayed on top of ECOSTRESS evapotranspiration.\n\nET_subset = ET_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\nraster_map = ET_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ET_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ET_subset.quantile(0.02), ET_subset.quantile(0.98))\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nNow let’s look at the tree health points on top of ECOSTRESS surface temperature.\n\nST_subset = ST_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\nraster_map = ST_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ST_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_subset.quantile(0.02), ST_subset.quantile(0.98)),\n    title=\"ECOSTRESS Surface Temperature and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnd let’s look at tree health on top of soil moisture.\n\nSM_subset = SM_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\nraster_map = SM_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=SM_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(SM_subset.quantile(0.02), SM_subset.quantile(0.98)),\n    title=\"ECOSTRESS Soil Moisture and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnd here’s tree health on top of vegetation index.\n\nNDVI_subset = NDVI_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\nraster_map = NDVI_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=NDVI_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(NDVI_subset.quantile(0.02), NDVI_subset.quantile(0.98)),\n    title=\"ECOSTRESS Vegetation Index and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nTo match these datasets together, let’s interpolate the ECOSTRESS rasters for evapotranspiration, surface temperature, soil moisture, and vegetation index to the Nature Conservency tree locations and make these sampled remote sensing data new columns in our table of tree observations.\n\nTNC_fall_2020[\"ET\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(ET_subset),\n    nodata=np.nan,\n    affine=ET_subset.rio.transform()\n)\n\nTNC_fall_2020[\"ST\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(ST_subset),\n    nodata=np.nan,\n    affine=ST_subset.rio.transform()\n)\n\nTNC_fall_2020[\"SM\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(SM_subset),\n    nodata=np.nan,\n    affine=SM_subset.rio.transform()\n)\n\nTNC_fall_2020[\"NDVI\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(NDVI_subset),\n    nodata=np.nan,\n    affine=NDVI_subset.rio.transform()\n)\n\nTNC_fall_2020 = TNC_fall_2020[[\"health\", \"ET\", \"ST\", \"SM\", \"NDVI\", \"geometry\"]]\nTNC_fall_2020.head()\n\n\n\n\n  \n    \n      \n      health\n      ET\n      ST\n      SM\n      NDVI\n      geometry\n    \n  \n  \n    \n      0\n      good\n      0.379620\n      34.051858\n      0.095231\n      0.322466\n      POINT Z (737195.535 3823512.913 191.300)\n    \n    \n      1\n      good\n      0.382044\n      34.182116\n      0.094925\n      0.323227\n      POINT Z (737183.595 3823525.373 194.800)\n    \n    \n      2\n      poor\n      0.381920\n      34.179540\n      0.094932\n      0.323164\n      POINT Z (737184.237 3823525.408 194.200)\n    \n    \n      3\n      good\n      0.378730\n      34.101747\n      0.095152\n      0.320645\n      POINT Z (737182.283 3823532.980 192.800)\n    \n    \n      4\n      good\n      0.378280\n      34.085753\n      0.095204\n      0.319984\n      POINT Z (737174.601 3823536.391 188.900)\n    \n  \n\n\n\n\nLet’s plot the distributions of ECOSTRESS evapotranspiration for each of the tree health categories. The dead tree points match to a lower distribution of interpolated ECOSTRESS evapotranspiration estimates.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.ET,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Evapotranspiration (mm/day)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Evapotranspiration by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLet’s look at the distribution of ECOSTRESS surface temperature according to in situ tree health. The dead trees appear to have higher temperatures than other categories.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.ST,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Surface Temperature (Celsius)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Surface Temperature by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLooking at this again with ECOSTRESS soil moisture, the dead trees appear to be associated with lower soil moisture estimates.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.SM,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Soil Moisture ($m^3/m^3$)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Soil Moisture by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nAnd the range of ECOSTRESS vegetation index at the dead trees appears to be lower than other categories.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.NDVI,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Normalized Difference Vegetation Index\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Vegetation Index by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLet’s also approach this match-up the other way, by rasterizing the in situ point data. We’ll spatially bin counts of the tree health categories within each 70 m ECOSTRESS pixel and calculate the ratio of these category counts to the total count of observations in each pixel. Let’s start with rasterizing the total counts first.\n\nx_bins = np.sort(ET_subset.x)\nx_bins = np.append(x_bins, x_bins[-1] + (x_bins[-1] - x_bins[-2]))\ny_bins = np.sort(ET_subset.y)\ny_bins = np.insert(y_bins, 0, y_bins[0] + (y_bins[0] - y_bins[1]))\n\nx = TNC_fall_2020.geometry.apply(lambda point: point.x)\ny = TNC_fall_2020.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ncounts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ncounts_map = counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\",\n    tiles=\"OSM\",\n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title=\"Total Count of Tree Observations within Each 70 m ECOSTRESS Pixel\"\n)\n\nNow we’ll count the number of dead trees in each pixel and divide that by the total count to get a proportion of dead trees in each pixel. This proportion of dead trees appears to range from 5% to 25% across the rasterized image.\n\ndead_gdf = TNC_fall_2020[TNC_fall_2020.health == \"dead\"]\nx = dead_gdf.geometry.apply(lambda point: point.x)\ny = dead_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ndead_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ndead_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\ndead_proportion_image = dead_counts_image / counts_image\ndead_proportion_image.attrs = ET_subset.attrs\n\ndead_proportion_map = dead_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\",\n    tiles=\"OSM\",\n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\ndead_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe’ll do this same rasterization for good trees.\n\ngood_gdf = TNC_fall_2020[TNC_fall_2020.health == \"good\"]\nx = good_gdf.geometry.apply(lambda point: point.x)\ny = good_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ngood_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ngood_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\ngood_proportion_image = good_counts_image / counts_image\ngood_proportion_image.attrs = ET_subset.attrs\n\ngood_proportion_map = good_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\", \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\ngood_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\npoor_gdf = TNC_fall_2020[TNC_fall_2020.health == \"poor\"]\nx = poor_gdf.geometry.apply(lambda point: point.x)\ny = poor_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\npoor_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\npoor_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\npoor_proportion_image = poor_counts_image / counts_image\npoor_proportion_image.attrs = ET_subset.attrs\n\npoor_proportion_map = poor_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\", \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\npoor_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nNow let’s bring it all together by collapsing each of these images into columns of a table so we can analyze these matching data points.\n\nproportion_table = pd.DataFrame({\n    \"dead\": dead_proportion_image.values.ravel(), \n    \"poor\": poor_proportion_image.values.ravel(), \n    \"good\": good_proportion_image.values.ravel(), \n    \"ET\": ET_subset.values.ravel(),\n    \"ST\": ST_subset.values.ravel(),\n    \"SM\": SM_subset.values.ravel(),\n    \"NDVI\": NDVI_subset.values.ravel()\n}).dropna()\n\nproportion_table = proportion_table[np.abs(zscore(proportion_table.ET)) < 2]\nproportion_table.head()\n\n\n\n\n  \n    \n      \n      dead\n      poor\n      good\n      ET\n      ST\n      SM\n      NDVI\n    \n  \n  \n    \n      177\n      0.250000\n      0.050000\n      0.075000\n      0.374550\n      32.950012\n      0.099246\n      0.315343\n    \n    \n      179\n      0.114286\n      0.085714\n      0.057143\n      0.379082\n      32.250000\n      0.100922\n      0.343636\n    \n    \n      244\n      0.052632\n      0.289474\n      0.342105\n      0.357015\n      32.250000\n      0.100832\n      0.309682\n    \n    \n      310\n      0.041667\n      0.083333\n      0.250000\n      0.391165\n      32.570007\n      0.100025\n      0.352369\n    \n    \n      311\n      0.033333\n      0.016667\n      0.233333\n      0.371817\n      33.270020\n      0.098373\n      0.330158\n    \n  \n\n\n\n\nLet’s investigate the correlations between these variables with a correlogram. Focusing on dead trees, it seems that there are rather weak relationships between the dead tree observations and the evapotranspiration input variables, but there is an inverse correlation between the dead tree observations and the evapotranspiration estimate itself.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.heatmap(proportion_table.corr(), annot=True)\nplt.title(\"Correlogram of ECOSTRESS Products to Tree Health Proportions\")\nplt.show()\nplt.close(fig)\n\n\n\n\nWe can break down these correlation coefficients in to individual scatterplots to better see how each pair of variables line up together.\n\nfig = plt.figure()\nax = sns.pairplot(proportion_table)\nplt.title(\"Pair-Plot of ECOSTRESS Products to Tree Health Proportions\")\nplt.show()\nplt.close(fig)\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\nLet’s focus on the strongest relationship involving dead trees and ECOSTRESS evapotranspiration. We can visualize this inverse correlation with a scatterplot and decreasing trendline.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.regplot(x=proportion_table.dead, y=proportion_table.ET)\nax.set(ylabel=\"ECOSTRESS Evapotranspiration (mm/day)\", xlabel=\"Proportion of Dead Tree Observations\")\nplt.title(\"Scatterplot of ECOSTRESS Evapotranspiration to Proportion of Dead Tree Observations\")\nplt.show()\nplt.close(fig)"
  }
]